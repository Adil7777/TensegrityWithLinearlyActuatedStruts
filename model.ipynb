{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    \"\"\"Set all random seeds for reproducibility\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  \n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "set_seed(42)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>A0</th>\n",
       "      <th>A1</th>\n",
       "      <th>A2</th>\n",
       "      <th>A5</th>\n",
       "      <th>A6</th>\n",
       "      <th>A7</th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>Z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.02</td>\n",
       "      <td>1.87</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.45</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.629333</td>\n",
       "      <td>0.844733</td>\n",
       "      <td>1.169218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.046863</td>\n",
       "      <td>2.95</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.87</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.629225</td>\n",
       "      <td>0.844531</td>\n",
       "      <td>1.169302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.078058</td>\n",
       "      <td>2.95</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.07</td>\n",
       "      <td>2.16</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.629148</td>\n",
       "      <td>0.844372</td>\n",
       "      <td>1.169372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.109227</td>\n",
       "      <td>3.02</td>\n",
       "      <td>1.65</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.629085</td>\n",
       "      <td>0.844232</td>\n",
       "      <td>1.169426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.140454</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.07</td>\n",
       "      <td>2.30</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.629038</td>\n",
       "      <td>0.844166</td>\n",
       "      <td>1.169454</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       time    A0    A1    A2    A5    A6   A7         X         Y         Z\n",
       "0  0.000000  3.02  1.87  0.28  2.45  0.14  0.0 -0.629333  0.844733  1.169218\n",
       "1  0.046863  2.95  1.80  0.00  1.87  0.00  0.0 -0.629225  0.844531  1.169302\n",
       "2  0.078058  2.95  1.80  0.07  2.16  0.00  0.0 -0.629148  0.844372  1.169372\n",
       "3  0.109227  3.02  1.65  0.00  2.01  0.00  0.0 -0.629085  0.844232  1.169426\n",
       "4  0.140454  3.17  1.80  0.07  2.30  0.07  0.0 -0.629038  0.844166  1.169454"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('19JuneDataset.csv')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 18165 entries, 0 to 18164\n",
      "Data columns (total 10 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   time    18165 non-null  float64\n",
      " 1   A0      18165 non-null  float64\n",
      " 2   A1      18165 non-null  float64\n",
      " 3   A2      18165 non-null  float64\n",
      " 4   A5      18165 non-null  float64\n",
      " 5   A6      18165 non-null  float64\n",
      " 6   A7      18165 non-null  float64\n",
      " 7   X       18165 non-null  float64\n",
      " 8   Y       18165 non-null  float64\n",
      " 9   Z       18165 non-null  float64\n",
      "dtypes: float64(10)\n",
      "memory usage: 1.4 MB\n"
     ]
    }
   ],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = dataset[[\"X\", \"Y\", \"Z\"]], dataset[['A0', 'A1', 'A2', 'A5', 'A6', 'A7']]\n",
    "\n",
    "x_mean = X.iloc[:, 0].mean()\n",
    "\n",
    "x_std = X.iloc[:, 0].std()\n",
    "y_mean = X.iloc[:, 1].mean()\n",
    "y_std = X.iloc[:, 1].std()\n",
    "z_mean = X.iloc[:, 2].mean()\n",
    "z_std = X.iloc[:, 2].std()\n",
    "\n",
    "# Normalize each column\n",
    "X_norm = np.copy(X)\n",
    "X_norm[:, 0] = (X.iloc[:, 0] - x_mean) / x_std\n",
    "X_norm[:, 1] = (X.iloc[:, 1] - y_mean) / y_std\n",
    "X_norm[:, 2] = (X.iloc[:, 2] - z_mean) / z_std\n",
    "\n",
    "y_norm = y / 73.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_norm = torch.tensor(pd.DataFrame(X_norm).values, dtype=torch.float32)\n",
    "y_norm = torch.tensor(pd.DataFrame(y_norm).values, dtype=torch.float32)\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X_norm, y_norm, test_size=0.2, shuffle=True, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2064,  0.8040,  0.0440],\n",
       "        [-2.1598,  1.2789,  0.7274],\n",
       "        [ 0.4554,  0.7665,  0.8691],\n",
       "        ...,\n",
       "        [ 0.3993, -0.9969,  0.6394],\n",
       "        [-1.5094,  0.7090, -0.9503],\n",
       "        [-0.1380,  0.6910,  0.3788]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "\n",
    "# Create loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearlyActuatedStrutsXavier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearlyActuatedStrutsXavier, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(3, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 6),\n",
    "            nn.Sigmoid()  # Constrain to [0, 1]\n",
    "        )\n",
    "        \n",
    "        # Initialize weights with Xavier/Glorot initialization\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.model.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                torch.manual_seed(42) # Set seed for reproducibility\n",
    "                init.xavier_uniform_(m.weight)  # Xavier uniform initialization\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)      # Initialize biases to zero\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "\n",
    "class LinearlyActuatedStrutsHe(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearlyActuatedStrutsHe, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(3, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 6),\n",
    "            nn.Sigmoid()  # Constrain to [0, 1]\n",
    "        )\n",
    "        \n",
    "        # Initialize weights with Xavier/Glorot initialization\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.model.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                torch.manual_seed(42) # Set seed for reproducibility\n",
    "                init.kaiming_uniform_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class LinearlyActuatedStrutsHeCompact(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearlyActuatedStrutsHeCompact, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(3, 128),  # Input -> Hidden 1 (128 units)\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),  # Hidden 1 -> Hidden 2 (64 units)\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 6),    # Hidden 2 -> Output\n",
    "            nn.Sigmoid()         # Constrain to [0, 1]\n",
    "        )\n",
    "        \n",
    "        # Initialize weights with He initialization\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.model.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                torch.manual_seed(42)  # Fixed seed for reproducibility\n",
    "                init.kaiming_uniform_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "\n",
    "class DeepLinearlyActuatedStruts(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(DeepLinearlyActuatedStruts, self).__init__()\n",
    "            self.model = nn.Sequential(\n",
    "                # Input -> Hidden 1 (balanced expansion)\n",
    "                nn.Linear(3, 96),          # Wider first layer\n",
    "                nn.BatchNorm1d(96),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.2),          # Regularization\n",
    "                \n",
    "                # Hidden 1 -> Hidden 2 (feature consolidation)\n",
    "                nn.Linear(96, 192),\n",
    "                nn.BatchNorm1d(192),\n",
    "                nn.ReLU(),\n",
    "                \n",
    "                # Hidden 2 -> Hidden 3 (bottleneck)\n",
    "                nn.Linear(192, 96),        # Symmetric contraction\n",
    "                nn.BatchNorm1d(96),\n",
    "                nn.ReLU(),\n",
    "                \n",
    "                # Hidden 3 -> Hidden 4 (pre-output compression)\n",
    "                nn.Linear(96, 48),\n",
    "                nn.BatchNorm1d(48),\n",
    "                nn.ReLU(),\n",
    "                \n",
    "                # Output layer\n",
    "                nn.Linear(48, 6),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "            self._initialize_weights()\n",
    "\n",
    "        def _initialize_weights(self):\n",
    "            for m in self.model.modules():\n",
    "                if isinstance(m, nn.Linear):\n",
    "                    init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.zeros_(m.bias)\n",
    "\n",
    "        def forward(self, x):\n",
    "            return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    model_name: str = \"model\",\n",
    "    criterion: nn.Module = nn.MSELoss(),\n",
    "    optimizer: torch.optim.Optimizer = None,\n",
    "    lr: float = 1e-3,\n",
    "    num_epochs: int = 100,\n",
    "    scale_factor: float = 73.8,\n",
    "    save_best: bool = True,\n",
    "    verbose: bool = True,\n",
    "    l1_lambda: float = 0.0,  # L1 regularization strength\n",
    "    l2_lambda: float = 0.0   # L2 regularization strength\n",
    ") -> dict:\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    if optimizer is None:\n",
    "        # Use AdamW for built-in L2 (weight decay)\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=l2_lambda)\n",
    "    \n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'best_val_loss': float('inf'),\n",
    "        'best_model_state': None,\n",
    "        'epoch_best': 0\n",
    "    }\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            # Add L1 regularization (if enabled)\n",
    "            if l1_lambda > 0:\n",
    "                l1_reg = torch.tensor(0., device=device)\n",
    "                for param in model.parameters():\n",
    "                    l1_reg += torch.norm(param, 1)\n",
    "                loss += l1_lambda * l1_reg\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        # Validation (no regularization)\n",
    "        val_loss = 0.0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        # Record metrics\n",
    "        train_loss = train_loss / len(train_loader.dataset) * scale_factor\n",
    "        val_loss = val_loss / len(val_loader.dataset) * scale_factor\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        \n",
    "        # Update best model\n",
    "        if val_loss < history['best_val_loss']:\n",
    "            history['best_val_loss'] = val_loss\n",
    "            history['best_model_state'] = model.state_dict()\n",
    "            history['epoch_best'] = epoch + 1\n",
    "        \n",
    "        if verbose and (epoch % 10 == 0 or epoch == num_epochs - 1):\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs} | \"\n",
    "                  f\"Train Loss: {train_loss:.4f} | \"\n",
    "                  f\"Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    if save_best and history['best_model_state'] is not None:\n",
    "        torch.save(history['best_model_state'], f'best_{model_name}.pth')\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300 | Train Loss: 2.8134 | Val Loss: 1.1331\n",
      "Epoch 11/300 | Train Loss: 0.0385 | Val Loss: 0.0318\n",
      "Epoch 21/300 | Train Loss: 0.0252 | Val Loss: 0.0312\n",
      "Epoch 31/300 | Train Loss: 0.0200 | Val Loss: 0.0219\n",
      "Epoch 41/300 | Train Loss: 0.0133 | Val Loss: 0.0131\n",
      "Epoch 51/300 | Train Loss: 0.0110 | Val Loss: 0.0098\n",
      "Epoch 61/300 | Train Loss: 0.0104 | Val Loss: 0.0127\n",
      "Epoch 71/300 | Train Loss: 0.0092 | Val Loss: 0.0088\n",
      "Epoch 81/300 | Train Loss: 0.0090 | Val Loss: 0.0084\n",
      "Epoch 91/300 | Train Loss: 0.0102 | Val Loss: 0.0077\n",
      "Epoch 101/300 | Train Loss: 0.0071 | Val Loss: 0.0088\n",
      "Epoch 111/300 | Train Loss: 0.0065 | Val Loss: 0.0065\n",
      "Epoch 121/300 | Train Loss: 0.0059 | Val Loss: 0.0084\n",
      "Epoch 131/300 | Train Loss: 0.0059 | Val Loss: 0.0077\n",
      "Epoch 141/300 | Train Loss: 0.0054 | Val Loss: 0.0056\n",
      "Epoch 151/300 | Train Loss: 0.0055 | Val Loss: 0.0057\n",
      "Epoch 161/300 | Train Loss: 0.0053 | Val Loss: 0.0049\n",
      "Epoch 171/300 | Train Loss: 0.0050 | Val Loss: 0.0054\n",
      "Epoch 181/300 | Train Loss: 0.0051 | Val Loss: 0.0069\n",
      "Epoch 191/300 | Train Loss: 0.0048 | Val Loss: 0.0053\n",
      "Epoch 201/300 | Train Loss: 0.0047 | Val Loss: 0.0047\n",
      "Epoch 211/300 | Train Loss: 0.0046 | Val Loss: 0.0053\n",
      "Epoch 221/300 | Train Loss: 0.0045 | Val Loss: 0.0050\n",
      "Epoch 231/300 | Train Loss: 0.0049 | Val Loss: 0.0044\n",
      "Epoch 241/300 | Train Loss: 0.0042 | Val Loss: 0.0049\n",
      "Epoch 251/300 | Train Loss: 0.0046 | Val Loss: 0.0045\n",
      "Epoch 261/300 | Train Loss: 0.0043 | Val Loss: 0.0060\n",
      "Epoch 271/300 | Train Loss: 0.0044 | Val Loss: 0.0042\n",
      "Epoch 281/300 | Train Loss: 0.0043 | Val Loss: 0.0050\n",
      "Epoch 291/300 | Train Loss: 0.0041 | Val Loss: 0.0045\n",
      "Epoch 300/300 | Train Loss: 0.0043 | Val Loss: 0.0042\n",
      "Best validation loss with Xavier Initialization: 0.0039 at epoch 297\n"
     ]
    }
   ],
   "source": [
    "# Initialize your model\n",
    "model_xavier = LinearlyActuatedStrutsXavier()\n",
    "\n",
    "# Train and evaluate\n",
    "history_xavier = train_model(\n",
    "    model=model_xavier,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    model_name=\"Xavier_Model_1e-3\",\n",
    "    num_epochs=300,\n",
    "    scale_factor=73.8  # Your specific scaling factor\n",
    ")\n",
    "\n",
    "# Access results\n",
    "print(f\"Best validation loss with Xavier Initialization: {history_xavier['best_val_loss']:.4f} at epoch {history_xavier['epoch_best']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_losses(train_loss, val_loss):\n",
    "    epochs = np.arange(1, len(train_loss) + 1)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(epochs, train_loss, label='Train Loss')\n",
    "    plt.plot(epochs, val_loss, label='Validation Loss')\n",
    "    plt.title('Losses over Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_losses(history_xavier[\"train_loss\"], history_xavier[\"val_loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Xavier_Model_1e-3 | MAE: 0.3312\n"
     ]
    }
   ],
   "source": [
    "def calculate_model_performance(model_name):\n",
    "    \"\"\"Calculate and print model performance metrics\"\"\"\n",
    "    if \"Xavier\" in model_name:\n",
    "        model = LinearlyActuatedStrutsXavier()\n",
    "    elif \"Compact\" in model_name:\n",
    "        model = LinearlyActuatedStrutsHeCompact()\n",
    "    elif \"Deep\" in model_name:\n",
    "        model = DeepLinearlyActuatedStruts()\n",
    "    else:\n",
    "        model = LinearlyActuatedStrutsHe()\n",
    "    model.load_state_dict(torch.load(f'best_{model_name}.pth'))\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(X_test)\n",
    "    \n",
    "    # Rescale predictions back to [0, 73.8]\n",
    "    pred_real = predictions * 73.8\n",
    "    \n",
    "    # Calculate metrics\n",
    "    # mse = nn.L1Loss()(pred_real, y_test * 73.8).item()\n",
    "    mae = torch.mean(torch.abs(pred_real - y_test * 73.8)).item()\n",
    "    \n",
    "    print(f\"Model: {model_name} | MAE: {mae:.4f}\")\n",
    "    return pred_real, mae\n",
    "\n",
    "\n",
    "xavier_pred, xavier_mae = calculate_model_performance(\"Xavier_Model_1e-3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[73.7000, 73.8000, 73.8000,  0.2000, 73.5000, 56.5000],\n",
      "        [14.9000, 73.8000, 73.4000, 73.0000, 73.5000, 73.6000],\n",
      "        [73.7000, 73.7000, 73.8000, 73.2000, 68.1000,  0.1000],\n",
      "        ...,\n",
      "        [73.4000, 73.6000, 73.7000, 36.4000,  0.1000,  0.1000],\n",
      "        [73.5000, 73.6000, 73.8000, 72.8000,  5.5000,  0.0000],\n",
      "        [73.4000, 73.5000, 73.7000, 63.2000,  0.0000,  0.0000]])\n"
     ]
    }
   ],
   "source": [
    "xavier_pred = torch.round(xavier_pred, decimals=1)\n",
    "print(xavier_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[73.8000, 73.8000, 73.8000,  0.1400, 73.2900, 55.1800],\n",
      "        [13.4900, 73.8000, 73.8000, 73.2200, 73.8000, 73.8000],\n",
      "        [73.8000, 73.8000, 73.8000, 73.2200, 67.5900,  0.0000],\n",
      "        ...,\n",
      "        [73.8000, 73.8000, 73.8000, 36.3500,  0.0000,  0.0000],\n",
      "        [73.5800, 73.5100, 73.6500, 72.5700,  4.3200,  0.0000],\n",
      "        [73.8000, 73.8000, 73.8000, 63.2600,  0.0000,  0.0000]])\n"
     ]
    }
   ],
   "source": [
    "y_test_scaled = y_test * 73.8\n",
    "print(y_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will try with smaller learning rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/600 | Train Loss: 3.9927 | Val Loss: 1.5046\n",
      "Epoch 11/600 | Train Loss: 0.0579 | Val Loss: 0.0529\n",
      "Epoch 21/600 | Train Loss: 0.0286 | Val Loss: 0.0276\n",
      "Epoch 31/600 | Train Loss: 0.0244 | Val Loss: 0.0234\n",
      "Epoch 41/600 | Train Loss: 0.0179 | Val Loss: 0.0235\n",
      "Epoch 51/600 | Train Loss: 0.0161 | Val Loss: 0.0151\n",
      "Epoch 61/600 | Train Loss: 0.0146 | Val Loss: 0.0129\n",
      "Epoch 71/600 | Train Loss: 0.0135 | Val Loss: 0.0111\n",
      "Epoch 81/600 | Train Loss: 0.0111 | Val Loss: 0.0104\n",
      "Epoch 91/600 | Train Loss: 0.0104 | Val Loss: 0.0103\n",
      "Epoch 101/600 | Train Loss: 0.0094 | Val Loss: 0.0085\n",
      "Epoch 111/600 | Train Loss: 0.0092 | Val Loss: 0.0087\n",
      "Epoch 121/600 | Train Loss: 0.0091 | Val Loss: 0.0072\n",
      "Epoch 131/600 | Train Loss: 0.0082 | Val Loss: 0.0115\n",
      "Epoch 141/600 | Train Loss: 0.0087 | Val Loss: 0.0078\n",
      "Epoch 151/600 | Train Loss: 0.0074 | Val Loss: 0.0079\n",
      "Epoch 161/600 | Train Loss: 0.0073 | Val Loss: 0.0198\n",
      "Epoch 171/600 | Train Loss: 0.0066 | Val Loss: 0.0075\n",
      "Epoch 181/600 | Train Loss: 0.0063 | Val Loss: 0.0063\n",
      "Epoch 191/600 | Train Loss: 0.0065 | Val Loss: 0.0059\n",
      "Epoch 201/600 | Train Loss: 0.0061 | Val Loss: 0.0059\n",
      "Epoch 211/600 | Train Loss: 0.0059 | Val Loss: 0.0059\n",
      "Epoch 221/600 | Train Loss: 0.0053 | Val Loss: 0.0062\n",
      "Epoch 231/600 | Train Loss: 0.0054 | Val Loss: 0.0053\n",
      "Epoch 241/600 | Train Loss: 0.0054 | Val Loss: 0.0093\n",
      "Epoch 251/600 | Train Loss: 0.0054 | Val Loss: 0.0065\n",
      "Epoch 261/600 | Train Loss: 0.0053 | Val Loss: 0.0053\n",
      "Epoch 271/600 | Train Loss: 0.0054 | Val Loss: 0.0057\n",
      "Epoch 281/600 | Train Loss: 0.0054 | Val Loss: 0.0059\n",
      "Epoch 291/600 | Train Loss: 0.0049 | Val Loss: 0.0066\n",
      "Epoch 301/600 | Train Loss: 0.0051 | Val Loss: 0.0061\n",
      "Epoch 311/600 | Train Loss: 0.0048 | Val Loss: 0.0062\n",
      "Epoch 321/600 | Train Loss: 0.0052 | Val Loss: 0.0046\n",
      "Epoch 331/600 | Train Loss: 0.0048 | Val Loss: 0.0059\n",
      "Epoch 341/600 | Train Loss: 0.0049 | Val Loss: 0.0056\n",
      "Epoch 351/600 | Train Loss: 0.0045 | Val Loss: 0.0047\n",
      "Epoch 361/600 | Train Loss: 0.0048 | Val Loss: 0.0053\n",
      "Epoch 371/600 | Train Loss: 0.0055 | Val Loss: 0.0048\n",
      "Epoch 381/600 | Train Loss: 0.0044 | Val Loss: 0.0049\n",
      "Epoch 391/600 | Train Loss: 0.0044 | Val Loss: 0.0049\n",
      "Epoch 401/600 | Train Loss: 0.0046 | Val Loss: 0.0047\n",
      "Epoch 411/600 | Train Loss: 0.0043 | Val Loss: 0.0053\n",
      "Epoch 421/600 | Train Loss: 0.0044 | Val Loss: 0.0049\n",
      "Epoch 431/600 | Train Loss: 0.0045 | Val Loss: 0.0045\n",
      "Epoch 441/600 | Train Loss: 0.0042 | Val Loss: 0.0053\n",
      "Epoch 451/600 | Train Loss: 0.0042 | Val Loss: 0.0043\n",
      "Epoch 461/600 | Train Loss: 0.0043 | Val Loss: 0.0043\n",
      "Epoch 471/600 | Train Loss: 0.0041 | Val Loss: 0.0047\n",
      "Epoch 481/600 | Train Loss: 0.0041 | Val Loss: 0.0045\n",
      "Epoch 491/600 | Train Loss: 0.0041 | Val Loss: 0.0045\n",
      "Epoch 501/600 | Train Loss: 0.0040 | Val Loss: 0.0042\n",
      "Epoch 511/600 | Train Loss: 0.0040 | Val Loss: 0.0047\n",
      "Epoch 521/600 | Train Loss: 0.0040 | Val Loss: 0.0045\n",
      "Epoch 531/600 | Train Loss: 0.0039 | Val Loss: 0.0041\n",
      "Epoch 541/600 | Train Loss: 0.0039 | Val Loss: 0.0041\n",
      "Epoch 551/600 | Train Loss: 0.0039 | Val Loss: 0.0054\n",
      "Epoch 561/600 | Train Loss: 0.0038 | Val Loss: 0.0043\n",
      "Epoch 571/600 | Train Loss: 0.0038 | Val Loss: 0.0042\n",
      "Epoch 581/600 | Train Loss: 0.0038 | Val Loss: 0.0046\n",
      "Epoch 591/600 | Train Loss: 0.0038 | Val Loss: 0.0040\n",
      "Epoch 600/600 | Train Loss: 0.0038 | Val Loss: 0.0042\n",
      "Best validation loss with Xavier Initialization and Learning rate of 5e-4: 0.0038 at epoch 562\n"
     ]
    }
   ],
   "source": [
    "model_xavier_2 = LinearlyActuatedStrutsXavier()\n",
    "\n",
    "# Train and evaluate\n",
    "history_xavier_2 = train_model(\n",
    "    model=model_xavier_2,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    model_name=\"Xavier_Model_5e-4\",\n",
    "    num_epochs=600,\n",
    "    scale_factor=73.8,  # Your specific scaling factor\n",
    "    lr= 5e-4,  # Learning rate for the second model\n",
    ")\n",
    "\n",
    "# Access results\n",
    "print(f\"Best validation loss with Xavier Initialization and Learning rate of 5e-4: {history_xavier_2['best_val_loss']:.4f} at epoch {history_xavier_2['epoch_best']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Xavier_Model_5e-4 | MAE: 0.3369\n"
     ]
    }
   ],
   "source": [
    "xavier_pred_2, xavier_mae_2 = calculate_model_performance(\"Xavier_Model_5e-4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will try with He initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/700 | Train Loss: 2.4090 | Val Loss: 1.0830\n",
      "Epoch 11/700 | Train Loss: 0.0388 | Val Loss: 0.0485\n",
      "Epoch 21/700 | Train Loss: 0.0246 | Val Loss: 0.0213\n",
      "Epoch 31/700 | Train Loss: 0.0141 | Val Loss: 0.0126\n",
      "Epoch 41/700 | Train Loss: 0.0098 | Val Loss: 0.0078\n",
      "Epoch 51/700 | Train Loss: 0.0079 | Val Loss: 0.0077\n",
      "Epoch 61/700 | Train Loss: 0.0071 | Val Loss: 0.0065\n",
      "Epoch 71/700 | Train Loss: 0.0138 | Val Loss: 0.0419\n",
      "Epoch 81/700 | Train Loss: 0.0062 | Val Loss: 0.0079\n",
      "Epoch 91/700 | Train Loss: 0.0066 | Val Loss: 0.0064\n",
      "Epoch 101/700 | Train Loss: 0.0051 | Val Loss: 0.0052\n",
      "Epoch 111/700 | Train Loss: 0.0050 | Val Loss: 0.0048\n",
      "Epoch 121/700 | Train Loss: 0.0053 | Val Loss: 0.0061\n",
      "Epoch 131/700 | Train Loss: 0.0047 | Val Loss: 0.0058\n",
      "Epoch 141/700 | Train Loss: 0.0047 | Val Loss: 0.0046\n",
      "Epoch 151/700 | Train Loss: 0.0046 | Val Loss: 0.0049\n",
      "Epoch 161/700 | Train Loss: 0.0053 | Val Loss: 0.0050\n",
      "Epoch 171/700 | Train Loss: 0.0041 | Val Loss: 0.0047\n",
      "Epoch 181/700 | Train Loss: 0.0043 | Val Loss: 0.0045\n",
      "Epoch 191/700 | Train Loss: 0.0045 | Val Loss: 0.0042\n",
      "Epoch 201/700 | Train Loss: 0.0075 | Val Loss: 0.0048\n",
      "Epoch 211/700 | Train Loss: 0.0041 | Val Loss: 0.0043\n",
      "Epoch 221/700 | Train Loss: 0.0041 | Val Loss: 0.0046\n",
      "Epoch 231/700 | Train Loss: 0.0045 | Val Loss: 0.0041\n",
      "Epoch 241/700 | Train Loss: 0.0038 | Val Loss: 0.0045\n",
      "Epoch 251/700 | Train Loss: 0.0040 | Val Loss: 0.0041\n",
      "Epoch 261/700 | Train Loss: 0.0039 | Val Loss: 0.0052\n",
      "Epoch 271/700 | Train Loss: 0.0040 | Val Loss: 0.0042\n",
      "Epoch 281/700 | Train Loss: 0.0039 | Val Loss: 0.0041\n",
      "Epoch 291/700 | Train Loss: 0.0038 | Val Loss: 0.0040\n",
      "Epoch 301/700 | Train Loss: 0.0037 | Val Loss: 0.0043\n",
      "Epoch 311/700 | Train Loss: 0.0036 | Val Loss: 0.0041\n",
      "Epoch 321/700 | Train Loss: 0.0036 | Val Loss: 0.0038\n",
      "Epoch 331/700 | Train Loss: 0.0038 | Val Loss: 0.0043\n",
      "Epoch 341/700 | Train Loss: 0.0038 | Val Loss: 0.0042\n",
      "Epoch 351/700 | Train Loss: 0.0039 | Val Loss: 0.0037\n",
      "Epoch 361/700 | Train Loss: 0.0036 | Val Loss: 0.0038\n",
      "Epoch 371/700 | Train Loss: 0.0036 | Val Loss: 0.0035\n",
      "Epoch 381/700 | Train Loss: 0.0035 | Val Loss: 0.0042\n",
      "Epoch 391/700 | Train Loss: 0.0036 | Val Loss: 0.0036\n",
      "Epoch 401/700 | Train Loss: 0.0035 | Val Loss: 0.0038\n",
      "Epoch 411/700 | Train Loss: 0.0034 | Val Loss: 0.0035\n",
      "Epoch 421/700 | Train Loss: 0.0035 | Val Loss: 0.0038\n",
      "Epoch 431/700 | Train Loss: 0.0034 | Val Loss: 0.0036\n",
      "Epoch 441/700 | Train Loss: 0.0038 | Val Loss: 0.0033\n",
      "Epoch 451/700 | Train Loss: 0.0035 | Val Loss: 0.0041\n",
      "Epoch 461/700 | Train Loss: 0.0035 | Val Loss: 0.0034\n",
      "Epoch 471/700 | Train Loss: 0.0035 | Val Loss: 0.0038\n",
      "Epoch 481/700 | Train Loss: 0.0036 | Val Loss: 0.0039\n",
      "Epoch 491/700 | Train Loss: 0.0034 | Val Loss: 0.0040\n",
      "Epoch 501/700 | Train Loss: 0.0035 | Val Loss: 0.0037\n",
      "Epoch 511/700 | Train Loss: 0.0035 | Val Loss: 0.0043\n",
      "Epoch 521/700 | Train Loss: 0.0034 | Val Loss: 0.0041\n",
      "Epoch 531/700 | Train Loss: 0.0033 | Val Loss: 0.0037\n",
      "Epoch 541/700 | Train Loss: 0.0034 | Val Loss: 0.0036\n",
      "Epoch 551/700 | Train Loss: 0.0034 | Val Loss: 0.0040\n",
      "Epoch 561/700 | Train Loss: 0.0033 | Val Loss: 0.0038\n",
      "Epoch 571/700 | Train Loss: 0.0034 | Val Loss: 0.0039\n",
      "Epoch 581/700 | Train Loss: 0.0033 | Val Loss: 0.0037\n",
      "Epoch 591/700 | Train Loss: 0.0033 | Val Loss: 0.0034\n",
      "Epoch 601/700 | Train Loss: 0.0034 | Val Loss: 0.0040\n",
      "Epoch 611/700 | Train Loss: 0.0033 | Val Loss: 0.0039\n",
      "Epoch 621/700 | Train Loss: 0.0034 | Val Loss: 0.0035\n",
      "Epoch 631/700 | Train Loss: 0.0034 | Val Loss: 0.0035\n",
      "Epoch 641/700 | Train Loss: 0.0032 | Val Loss: 0.0037\n",
      "Epoch 651/700 | Train Loss: 0.0033 | Val Loss: 0.0038\n",
      "Epoch 661/700 | Train Loss: 0.0032 | Val Loss: 0.0038\n",
      "Epoch 671/700 | Train Loss: 0.0032 | Val Loss: 0.0036\n",
      "Epoch 681/700 | Train Loss: 0.0033 | Val Loss: 0.0040\n",
      "Epoch 691/700 | Train Loss: 0.0033 | Val Loss: 0.0038\n",
      "Epoch 700/700 | Train Loss: 0.0033 | Val Loss: 0.0039\n",
      "Best validation loss with He Initialization: 0.0033 at epoch 692\n"
     ]
    }
   ],
   "source": [
    "model_he = LinearlyActuatedStrutsHe()\n",
    "# Train and evaluate\n",
    "history_he = train_model(\n",
    "    model=model_he,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    model_name=\"He_Model_1e-3\",\n",
    "    num_epochs=700,\n",
    "    scale_factor=73.8  # Your specific scaling factor\n",
    ")\n",
    "\n",
    "print(f\"Best validation loss with He Initialization: {history_he['best_val_loss']:.4f} at epoch {history_he['epoch_best']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: He_Model_1e-3 | MAE: 0.2949\n",
      "tensor([[73.8000, 73.8000, 73.8000,  0.1000, 73.5000, 56.8000],\n",
      "        [14.0000, 73.8000, 73.4000, 72.9000, 73.3000, 73.8000],\n",
      "        [73.6000, 73.7000, 73.7000, 73.4000, 67.8000,  0.0000],\n",
      "        ...,\n",
      "        [73.4000, 73.6000, 73.7000, 36.3000,  0.0000,  0.0000],\n",
      "        [73.6000, 73.6000, 73.7000, 72.8000,  4.2000,  0.0000],\n",
      "        [73.4000, 73.6000, 73.7000, 62.9000,  0.0000,  0.0000]])\n"
     ]
    }
   ],
   "source": [
    "he_pred, he_mae = calculate_model_performance(\"He_Model_1e-3\")\n",
    "\n",
    "he_pred = torch.round(he_pred, decimals=1)\n",
    "print(he_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/600 | Train Loss: 3.2758 | Val Loss: 1.4228\n",
      "Epoch 11/600 | Train Loss: 0.0537 | Val Loss: 0.0423\n",
      "Epoch 21/600 | Train Loss: 0.0260 | Val Loss: 0.0342\n",
      "Epoch 31/600 | Train Loss: 0.0213 | Val Loss: 0.0165\n",
      "Epoch 41/600 | Train Loss: 0.0152 | Val Loss: 0.0143\n",
      "Epoch 51/600 | Train Loss: 0.0117 | Val Loss: 0.0118\n",
      "Epoch 61/600 | Train Loss: 0.0112 | Val Loss: 0.0108\n",
      "Epoch 71/600 | Train Loss: 0.0098 | Val Loss: 0.0141\n",
      "Epoch 81/600 | Train Loss: 0.0078 | Val Loss: 0.0106\n",
      "Epoch 91/600 | Train Loss: 0.0070 | Val Loss: 0.0081\n",
      "Epoch 101/600 | Train Loss: 0.0069 | Val Loss: 0.0068\n",
      "Epoch 111/600 | Train Loss: 0.0069 | Val Loss: 0.0080\n",
      "Epoch 121/600 | Train Loss: 0.0061 | Val Loss: 0.0058\n",
      "Epoch 131/600 | Train Loss: 0.0062 | Val Loss: 0.0076\n",
      "Epoch 141/600 | Train Loss: 0.0055 | Val Loss: 0.0057\n",
      "Epoch 151/600 | Train Loss: 0.0054 | Val Loss: 0.0050\n",
      "Epoch 161/600 | Train Loss: 0.0050 | Val Loss: 0.0071\n",
      "Epoch 171/600 | Train Loss: 0.0052 | Val Loss: 0.0049\n",
      "Epoch 181/600 | Train Loss: 0.0050 | Val Loss: 0.0053\n",
      "Epoch 191/600 | Train Loss: 0.0051 | Val Loss: 0.0054\n",
      "Epoch 201/600 | Train Loss: 0.0048 | Val Loss: 0.0050\n",
      "Epoch 211/600 | Train Loss: 0.0047 | Val Loss: 0.0048\n",
      "Epoch 221/600 | Train Loss: 0.0046 | Val Loss: 0.0056\n",
      "Epoch 231/600 | Train Loss: 0.0048 | Val Loss: 0.0048\n",
      "Epoch 241/600 | Train Loss: 0.0044 | Val Loss: 0.0046\n",
      "Epoch 251/600 | Train Loss: 0.0044 | Val Loss: 0.0053\n",
      "Epoch 261/600 | Train Loss: 0.0043 | Val Loss: 0.0049\n",
      "Epoch 271/600 | Train Loss: 0.0042 | Val Loss: 0.0044\n",
      "Epoch 281/600 | Train Loss: 0.0044 | Val Loss: 0.0042\n",
      "Epoch 291/600 | Train Loss: 0.0040 | Val Loss: 0.0045\n",
      "Epoch 301/600 | Train Loss: 0.0041 | Val Loss: 0.0050\n",
      "Epoch 311/600 | Train Loss: 0.0040 | Val Loss: 0.0049\n",
      "Epoch 321/600 | Train Loss: 0.0039 | Val Loss: 0.0043\n",
      "Epoch 331/600 | Train Loss: 0.0041 | Val Loss: 0.0043\n",
      "Epoch 341/600 | Train Loss: 0.0040 | Val Loss: 0.0044\n",
      "Epoch 351/600 | Train Loss: 0.0039 | Val Loss: 0.0045\n",
      "Epoch 361/600 | Train Loss: 0.0040 | Val Loss: 0.0041\n",
      "Epoch 371/600 | Train Loss: 0.0041 | Val Loss: 0.0039\n",
      "Epoch 381/600 | Train Loss: 0.0038 | Val Loss: 0.0046\n",
      "Epoch 391/600 | Train Loss: 0.0039 | Val Loss: 0.0041\n",
      "Epoch 401/600 | Train Loss: 0.0038 | Val Loss: 0.0040\n",
      "Epoch 411/600 | Train Loss: 0.0037 | Val Loss: 0.0040\n",
      "Epoch 421/600 | Train Loss: 0.0037 | Val Loss: 0.0042\n",
      "Epoch 431/600 | Train Loss: 0.0037 | Val Loss: 0.0039\n",
      "Epoch 441/600 | Train Loss: 0.0036 | Val Loss: 0.0044\n",
      "Epoch 451/600 | Train Loss: 0.0037 | Val Loss: 0.0038\n",
      "Epoch 461/600 | Train Loss: 0.0036 | Val Loss: 0.0037\n",
      "Epoch 471/600 | Train Loss: 0.0037 | Val Loss: 0.0039\n",
      "Epoch 481/600 | Train Loss: 0.0036 | Val Loss: 0.0044\n",
      "Epoch 491/600 | Train Loss: 0.0036 | Val Loss: 0.0041\n",
      "Epoch 501/600 | Train Loss: 0.0035 | Val Loss: 0.0039\n",
      "Epoch 511/600 | Train Loss: 0.0035 | Val Loss: 0.0035\n",
      "Epoch 521/600 | Train Loss: 0.0034 | Val Loss: 0.0040\n",
      "Epoch 531/600 | Train Loss: 0.0034 | Val Loss: 0.0037\n",
      "Epoch 541/600 | Train Loss: 0.0035 | Val Loss: 0.0035\n",
      "Epoch 551/600 | Train Loss: 0.0034 | Val Loss: 0.0046\n",
      "Epoch 561/600 | Train Loss: 0.0035 | Val Loss: 0.0040\n",
      "Epoch 571/600 | Train Loss: 0.0034 | Val Loss: 0.0047\n",
      "Epoch 581/600 | Train Loss: 0.0033 | Val Loss: 0.0036\n",
      "Epoch 591/600 | Train Loss: 0.0033 | Val Loss: 0.0035\n",
      "Epoch 600/600 | Train Loss: 0.0034 | Val Loss: 0.0041\n",
      "Best validation loss with He Initialization and Learning rate of 5e-4: 0.0035 at epoch 541\n"
     ]
    }
   ],
   "source": [
    "he_model_2 = LinearlyActuatedStrutsHe()\n",
    "# Train and evaluate\n",
    "history_he_2 = train_model(\n",
    "    model=he_model_2,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    model_name=\"He_Model_5e-4\",\n",
    "    num_epochs=600,\n",
    "    scale_factor=73.8,  # Your specific scaling factor\n",
    "    lr= 5e-4,  # Learning rate for the second model\n",
    ")\n",
    "\n",
    "print(f\"Best validation loss with He Initialization and Learning rate of 5e-4: {history_he_2['best_val_loss']:.4f} at epoch {history_he_2['epoch_best']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: He_Model_5e-4 | MAE: 0.3238\n",
      "tensor([[73.7000, 73.8000, 73.8000,  0.1000, 73.6000, 56.5000],\n",
      "        [14.5000, 73.8000, 73.3000, 72.9000, 73.5000, 73.6000],\n",
      "        [73.6000, 73.6000, 73.6000, 73.2000, 67.5000,  0.1000],\n",
      "        ...,\n",
      "        [73.4000, 73.6000, 73.7000, 35.2000,  0.0000,  0.0000],\n",
      "        [73.6000, 73.5000, 73.7000, 72.8000,  4.7000,  0.0000],\n",
      "        [73.4000, 73.5000, 73.7000, 62.8000,  0.0000,  0.0000]])\n"
     ]
    }
   ],
   "source": [
    "he_pred_2, he_mae_2 = calculate_model_performance(\"He_Model_5e-4\")\n",
    "\n",
    "he_pred_2 = torch.round(he_pred_2, decimals=1)\n",
    "print(he_pred_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/600 | Train Loss: 1.5473 | Val Loss: 0.1270\n",
      "Epoch 11/600 | Train Loss: 0.0274 | Val Loss: 0.0216\n",
      "Epoch 21/600 | Train Loss: 0.0126 | Val Loss: 0.0141\n",
      "Epoch 31/600 | Train Loss: 0.0165 | Val Loss: 0.0080\n",
      "Epoch 41/600 | Train Loss: 0.0094 | Val Loss: 0.0083\n",
      "Epoch 51/600 | Train Loss: 0.0075 | Val Loss: 0.0075\n",
      "Epoch 61/600 | Train Loss: 0.0075 | Val Loss: 0.0101\n",
      "Epoch 71/600 | Train Loss: 0.0115 | Val Loss: 0.0094\n",
      "Epoch 81/600 | Train Loss: 0.0074 | Val Loss: 0.0173\n",
      "Epoch 91/600 | Train Loss: 0.0061 | Val Loss: 0.0105\n",
      "Epoch 101/600 | Train Loss: 0.0112 | Val Loss: 0.0060\n",
      "Epoch 111/600 | Train Loss: 0.0062 | Val Loss: 0.0061\n",
      "Epoch 121/600 | Train Loss: 0.0061 | Val Loss: 0.0064\n",
      "Epoch 131/600 | Train Loss: 0.0086 | Val Loss: 0.0070\n",
      "Epoch 141/600 | Train Loss: 0.0056 | Val Loss: 0.0057\n",
      "Epoch 151/600 | Train Loss: 0.0062 | Val Loss: 0.0064\n",
      "Epoch 161/600 | Train Loss: 0.0054 | Val Loss: 0.0056\n",
      "Epoch 171/600 | Train Loss: 0.0074 | Val Loss: 0.0074\n",
      "Epoch 181/600 | Train Loss: 0.0054 | Val Loss: 0.0059\n",
      "Epoch 191/600 | Train Loss: 0.0057 | Val Loss: 0.0059\n",
      "Epoch 201/600 | Train Loss: 0.0052 | Val Loss: 0.0063\n",
      "Epoch 211/600 | Train Loss: 0.0051 | Val Loss: 0.0053\n",
      "Epoch 221/600 | Train Loss: 0.0060 | Val Loss: 0.0052\n",
      "Epoch 231/600 | Train Loss: 0.0057 | Val Loss: 0.0053\n",
      "Epoch 241/600 | Train Loss: 0.0052 | Val Loss: 0.0055\n",
      "Epoch 251/600 | Train Loss: 0.0074 | Val Loss: 0.0067\n",
      "Epoch 261/600 | Train Loss: 0.0054 | Val Loss: 0.0055\n",
      "Epoch 271/600 | Train Loss: 0.0061 | Val Loss: 0.0057\n",
      "Epoch 281/600 | Train Loss: 0.0052 | Val Loss: 0.0060\n",
      "Epoch 291/600 | Train Loss: 0.0053 | Val Loss: 0.0063\n",
      "Epoch 301/600 | Train Loss: 0.0049 | Val Loss: 0.0048\n",
      "Epoch 311/600 | Train Loss: 0.0048 | Val Loss: 0.0058\n",
      "Epoch 321/600 | Train Loss: 0.0051 | Val Loss: 0.0052\n",
      "Epoch 331/600 | Train Loss: 0.0059 | Val Loss: 0.0056\n",
      "Epoch 341/600 | Train Loss: 0.0061 | Val Loss: 0.0060\n",
      "Epoch 351/600 | Train Loss: 0.0051 | Val Loss: 0.0047\n",
      "Epoch 361/600 | Train Loss: 0.0061 | Val Loss: 0.0057\n",
      "Epoch 371/600 | Train Loss: 0.0057 | Val Loss: 0.0048\n",
      "Epoch 381/600 | Train Loss: 0.0055 | Val Loss: 0.0076\n",
      "Epoch 391/600 | Train Loss: 0.0050 | Val Loss: 0.0053\n",
      "Epoch 401/600 | Train Loss: 0.0057 | Val Loss: 0.0050\n",
      "Epoch 411/600 | Train Loss: 0.0052 | Val Loss: 0.0055\n",
      "Epoch 421/600 | Train Loss: 0.0054 | Val Loss: 0.0166\n",
      "Epoch 431/600 | Train Loss: 0.0052 | Val Loss: 0.0049\n",
      "Epoch 441/600 | Train Loss: 0.0065 | Val Loss: 0.0064\n",
      "Epoch 451/600 | Train Loss: 0.0054 | Val Loss: 0.0055\n",
      "Epoch 461/600 | Train Loss: 0.0055 | Val Loss: 0.0052\n",
      "Epoch 471/600 | Train Loss: 0.0063 | Val Loss: 0.0091\n",
      "Epoch 481/600 | Train Loss: 0.0061 | Val Loss: 0.0055\n",
      "Epoch 491/600 | Train Loss: 0.0062 | Val Loss: 0.0053\n",
      "Epoch 501/600 | Train Loss: 0.0069 | Val Loss: 0.0067\n",
      "Epoch 511/600 | Train Loss: 0.0048 | Val Loss: 0.0054\n",
      "Epoch 521/600 | Train Loss: 0.0051 | Val Loss: 0.0058\n",
      "Epoch 531/600 | Train Loss: 0.0052 | Val Loss: 0.0050\n",
      "Epoch 541/600 | Train Loss: 0.0055 | Val Loss: 0.0047\n",
      "Epoch 551/600 | Train Loss: 0.0050 | Val Loss: 0.0052\n",
      "Epoch 561/600 | Train Loss: 0.0049 | Val Loss: 0.0046\n",
      "Epoch 571/600 | Train Loss: 0.0049 | Val Loss: 0.0076\n",
      "Epoch 581/600 | Train Loss: 0.0049 | Val Loss: 0.0052\n",
      "Epoch 591/600 | Train Loss: 0.0084 | Val Loss: 0.0081\n",
      "Epoch 600/600 | Train Loss: 0.0049 | Val Loss: 0.0046\n",
      "Best validation loss with He Initialization and Learning rate of 5e-3: 0.0043 at epoch 562\n"
     ]
    }
   ],
   "source": [
    "he_model_3 = LinearlyActuatedStrutsHe()\n",
    "# Train and evaluate\n",
    "history_he_3 = train_model(\n",
    "    model=he_model_3,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    model_name=\"He_Model_5e-3\",\n",
    "    num_epochs=600,\n",
    "    scale_factor=73.8,  # Your specific scaling factor\n",
    "    lr= 5e-3,  # Learning rate for the second model\n",
    ")\n",
    "\n",
    "print(f\"Best validation loss with He Initialization and Learning rate of 5e-3: {history_he_3['best_val_loss']:.4f} at epoch {history_he_3['epoch_best']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: He_Model_5e-3 | MAE: 0.3278\n",
      "tensor([[73.8000, 73.8000, 73.8000,  0.0000, 73.5000, 56.7000],\n",
      "        [13.0000, 73.6000, 73.7000, 72.8000, 73.4000, 73.8000],\n",
      "        [73.7000, 73.7000, 73.8000, 73.3000, 67.5000,  0.0000],\n",
      "        ...,\n",
      "        [73.5000, 73.5000, 73.7000, 36.1000,  0.0000,  0.0000],\n",
      "        [73.5000, 73.7000, 73.7000, 72.6000,  4.8000,  0.0000],\n",
      "        [73.3000, 73.6000, 73.6000, 62.8000,  0.0000,  0.0000]])\n"
     ]
    }
   ],
   "source": [
    "he_model_3_pred_3, he_mae_3 = calculate_model_performance(\"He_Model_5e-3\")\n",
    "he_model_3_pred_3 = torch.round(he_model_3_pred_3, decimals=1)\n",
    "print(he_model_3_pred_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/600 | Train Loss: 2.4109 | Val Loss: 1.0850\n",
      "Epoch 11/600 | Train Loss: 0.0364 | Val Loss: 0.0327\n",
      "Epoch 21/600 | Train Loss: 0.0214 | Val Loss: 0.0168\n",
      "Epoch 31/600 | Train Loss: 0.0130 | Val Loss: 0.0106\n",
      "Epoch 41/600 | Train Loss: 0.0087 | Val Loss: 0.0070\n",
      "Epoch 51/600 | Train Loss: 0.0089 | Val Loss: 0.0100\n",
      "Epoch 61/600 | Train Loss: 0.0072 | Val Loss: 0.0077\n",
      "Epoch 71/600 | Train Loss: 0.0193 | Val Loss: 0.0430\n",
      "Epoch 81/600 | Train Loss: 0.0061 | Val Loss: 0.0063\n",
      "Epoch 91/600 | Train Loss: 0.0059 | Val Loss: 0.0060\n",
      "Epoch 101/600 | Train Loss: 0.0053 | Val Loss: 0.0053\n",
      "Epoch 111/600 | Train Loss: 0.0050 | Val Loss: 0.0053\n",
      "Epoch 121/600 | Train Loss: 0.0046 | Val Loss: 0.0052\n",
      "Epoch 131/600 | Train Loss: 0.0047 | Val Loss: 0.0053\n",
      "Epoch 141/600 | Train Loss: 0.0048 | Val Loss: 0.0050\n",
      "Epoch 151/600 | Train Loss: 0.0052 | Val Loss: 0.0048\n",
      "Epoch 161/600 | Train Loss: 0.0047 | Val Loss: 0.0051\n",
      "Epoch 171/600 | Train Loss: 0.0043 | Val Loss: 0.0049\n",
      "Epoch 181/600 | Train Loss: 0.0042 | Val Loss: 0.0047\n",
      "Epoch 191/600 | Train Loss: 0.0044 | Val Loss: 0.0046\n",
      "Epoch 201/600 | Train Loss: 0.0046 | Val Loss: 0.0046\n",
      "Epoch 211/600 | Train Loss: 0.0044 | Val Loss: 0.0046\n",
      "Epoch 221/600 | Train Loss: 0.0044 | Val Loss: 0.0048\n",
      "Epoch 231/600 | Train Loss: 0.0044 | Val Loss: 0.0047\n",
      "Epoch 241/600 | Train Loss: 0.0043 | Val Loss: 0.0043\n",
      "Epoch 251/600 | Train Loss: 0.0042 | Val Loss: 0.0044\n",
      "Epoch 261/600 | Train Loss: 0.0048 | Val Loss: 0.0052\n",
      "Epoch 271/600 | Train Loss: 0.0041 | Val Loss: 0.0040\n",
      "Epoch 281/600 | Train Loss: 0.0044 | Val Loss: 0.0057\n",
      "Epoch 291/600 | Train Loss: 0.0041 | Val Loss: 0.0047\n",
      "Epoch 301/600 | Train Loss: 0.0042 | Val Loss: 0.0044\n",
      "Epoch 311/600 | Train Loss: 0.0040 | Val Loss: 0.0045\n",
      "Epoch 321/600 | Train Loss: 0.0039 | Val Loss: 0.0042\n",
      "Epoch 331/600 | Train Loss: 0.0040 | Val Loss: 0.0044\n",
      "Epoch 341/600 | Train Loss: 0.0040 | Val Loss: 0.0050\n",
      "Epoch 351/600 | Train Loss: 0.0040 | Val Loss: 0.0040\n",
      "Epoch 361/600 | Train Loss: 0.0080 | Val Loss: 0.0044\n",
      "Epoch 371/600 | Train Loss: 0.0040 | Val Loss: 0.0040\n",
      "Epoch 381/600 | Train Loss: 0.0039 | Val Loss: 0.0044\n",
      "Epoch 391/600 | Train Loss: 0.0040 | Val Loss: 0.0039\n",
      "Epoch 401/600 | Train Loss: 0.0040 | Val Loss: 0.0041\n",
      "Epoch 411/600 | Train Loss: 0.0042 | Val Loss: 0.0043\n",
      "Epoch 421/600 | Train Loss: 0.0039 | Val Loss: 0.0046\n",
      "Epoch 431/600 | Train Loss: 0.0039 | Val Loss: 0.0043\n",
      "Epoch 441/600 | Train Loss: 0.0042 | Val Loss: 0.0042\n",
      "Epoch 451/600 | Train Loss: 0.0040 | Val Loss: 0.0040\n",
      "Epoch 461/600 | Train Loss: 0.0038 | Val Loss: 0.0042\n",
      "Epoch 471/600 | Train Loss: 0.0039 | Val Loss: 0.0040\n",
      "Epoch 481/600 | Train Loss: 0.0038 | Val Loss: 0.0040\n",
      "Epoch 491/600 | Train Loss: 0.0040 | Val Loss: 0.0044\n",
      "Epoch 501/600 | Train Loss: 0.0040 | Val Loss: 0.0042\n",
      "Epoch 511/600 | Train Loss: 0.0036 | Val Loss: 0.0042\n",
      "Epoch 521/600 | Train Loss: 0.0038 | Val Loss: 0.0044\n",
      "Epoch 531/600 | Train Loss: 0.0040 | Val Loss: 0.0082\n",
      "Epoch 541/600 | Train Loss: 0.0039 | Val Loss: 0.0043\n",
      "Epoch 551/600 | Train Loss: 0.0038 | Val Loss: 0.0042\n",
      "Epoch 561/600 | Train Loss: 0.0039 | Val Loss: 0.0042\n",
      "Epoch 571/600 | Train Loss: 0.0039 | Val Loss: 0.0042\n",
      "Epoch 581/600 | Train Loss: 0.0039 | Val Loss: 0.0041\n",
      "Epoch 591/600 | Train Loss: 0.0038 | Val Loss: 0.0052\n",
      "Epoch 600/600 | Train Loss: 0.0037 | Val Loss: 0.0043\n",
      "Best validation loss with He Initialization and Learning rate decay: 0.0035 at epoch 541\n"
     ]
    }
   ],
   "source": [
    "he_lr_decay = LinearlyActuatedStrutsHe()\n",
    "\n",
    "he_lr_decay_history = train_model(\n",
    "    model=he_lr_decay,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    model_name=\"He_Model_LR_Decay\",\n",
    "    num_epochs=600,\n",
    "    scale_factor=73.8,  # Your specific scaling factor\n",
    "    # lr= 5e-4,  # Learning rate for the second model\n",
    "    optimizer = torch.optim.AdamW(he_lr_decay.parameters(), lr=1e-3, weight_decay=0.01)\n",
    ")\n",
    "print(f\"Best validation loss with He Initialization and Learning rate decay: {history_he_2['best_val_loss']:.4f} at epoch {history_he_2['epoch_best']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: He_Model_LR_Decay | MAE: 0.3319\n",
      "tensor([[73.7000, 73.8000, 73.8000,  0.1000, 73.5000, 56.7000],\n",
      "        [14.5000, 73.8000, 73.4000, 73.0000, 73.5000, 73.6000],\n",
      "        [73.6000, 73.6000, 73.7000, 73.4000, 67.6000,  0.0000],\n",
      "        ...,\n",
      "        [73.4000, 73.5000, 73.7000, 35.8000,  0.0000,  0.1000],\n",
      "        [73.5000, 73.5000, 73.6000, 72.8000,  4.7000,  0.1000],\n",
      "        [73.4000, 73.6000, 73.7000, 62.6000,  0.0000,  0.0000]])\n"
     ]
    }
   ],
   "source": [
    "he_decay_, he_decay_mae = calculate_model_performance(\"He_Model_LR_Decay\")\n",
    "\n",
    "he_decay_ = torch.round(he_decay_, decimals=1)\n",
    "print(he_decay_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/600 | Train Loss: 3.2627 | Val Loss: 1.7551\n",
      "Epoch 11/600 | Train Loss: 0.3843 | Val Loss: 0.3889\n",
      "Epoch 21/600 | Train Loss: 0.0312 | Val Loss: 0.0478\n",
      "Epoch 31/600 | Train Loss: 0.0259 | Val Loss: 0.0252\n",
      "Epoch 41/600 | Train Loss: 0.0174 | Val Loss: 0.0166\n",
      "Epoch 51/600 | Train Loss: 0.0161 | Val Loss: 0.0136\n",
      "Epoch 61/600 | Train Loss: 0.0179 | Val Loss: 0.0166\n",
      "Epoch 71/600 | Train Loss: 0.0139 | Val Loss: 0.0108\n",
      "Epoch 81/600 | Train Loss: 0.0108 | Val Loss: 0.0105\n",
      "Epoch 91/600 | Train Loss: 0.0101 | Val Loss: 0.0145\n",
      "Epoch 101/600 | Train Loss: 0.0089 | Val Loss: 0.0093\n",
      "Epoch 111/600 | Train Loss: 0.0090 | Val Loss: 0.0121\n",
      "Epoch 121/600 | Train Loss: 0.0099 | Val Loss: 0.0087\n",
      "Epoch 131/600 | Train Loss: 0.0077 | Val Loss: 0.0092\n",
      "Epoch 141/600 | Train Loss: 0.0081 | Val Loss: 0.0069\n",
      "Epoch 151/600 | Train Loss: 0.0073 | Val Loss: 0.0071\n",
      "Epoch 161/600 | Train Loss: 0.0072 | Val Loss: 0.0091\n",
      "Epoch 171/600 | Train Loss: 0.0064 | Val Loss: 0.0081\n",
      "Epoch 181/600 | Train Loss: 0.0069 | Val Loss: 0.0088\n",
      "Epoch 191/600 | Train Loss: 0.0067 | Val Loss: 0.0063\n",
      "Epoch 201/600 | Train Loss: 0.0067 | Val Loss: 0.0068\n",
      "Epoch 211/600 | Train Loss: 0.0060 | Val Loss: 0.0064\n",
      "Epoch 221/600 | Train Loss: 0.0059 | Val Loss: 0.0085\n",
      "Epoch 231/600 | Train Loss: 0.0065 | Val Loss: 0.0052\n",
      "Epoch 241/600 | Train Loss: 0.0058 | Val Loss: 0.0072\n",
      "Epoch 251/600 | Train Loss: 0.0055 | Val Loss: 0.0054\n",
      "Epoch 261/600 | Train Loss: 0.0058 | Val Loss: 0.0086\n",
      "Epoch 271/600 | Train Loss: 0.0055 | Val Loss: 0.0053\n",
      "Epoch 281/600 | Train Loss: 0.0058 | Val Loss: 0.0061\n",
      "Epoch 291/600 | Train Loss: 0.0052 | Val Loss: 0.0058\n",
      "Epoch 301/600 | Train Loss: 0.0056 | Val Loss: 0.0075\n",
      "Epoch 311/600 | Train Loss: 0.0052 | Val Loss: 0.0063\n",
      "Epoch 321/600 | Train Loss: 0.0052 | Val Loss: 0.0053\n",
      "Epoch 331/600 | Train Loss: 0.0050 | Val Loss: 0.0057\n",
      "Epoch 341/600 | Train Loss: 0.0052 | Val Loss: 0.0055\n",
      "Epoch 351/600 | Train Loss: 0.0062 | Val Loss: 0.0055\n",
      "Epoch 361/600 | Train Loss: 0.0051 | Val Loss: 0.0051\n",
      "Epoch 371/600 | Train Loss: 0.0051 | Val Loss: 0.0050\n",
      "Epoch 381/600 | Train Loss: 0.0048 | Val Loss: 0.0051\n",
      "Epoch 391/600 | Train Loss: 0.0051 | Val Loss: 0.0062\n",
      "Epoch 401/600 | Train Loss: 0.0049 | Val Loss: 0.0047\n",
      "Epoch 411/600 | Train Loss: 0.0048 | Val Loss: 0.0056\n",
      "Epoch 421/600 | Train Loss: 0.0048 | Val Loss: 0.0062\n",
      "Epoch 431/600 | Train Loss: 0.0047 | Val Loss: 0.0048\n",
      "Epoch 441/600 | Train Loss: 0.0046 | Val Loss: 0.0047\n",
      "Epoch 451/600 | Train Loss: 0.0047 | Val Loss: 0.0057\n",
      "Epoch 461/600 | Train Loss: 0.0045 | Val Loss: 0.0056\n",
      "Epoch 471/600 | Train Loss: 0.0045 | Val Loss: 0.0049\n",
      "Epoch 481/600 | Train Loss: 0.0047 | Val Loss: 0.0052\n",
      "Epoch 491/600 | Train Loss: 0.0045 | Val Loss: 0.0050\n",
      "Epoch 501/600 | Train Loss: 0.0045 | Val Loss: 0.0044\n",
      "Epoch 511/600 | Train Loss: 0.0045 | Val Loss: 0.0046\n",
      "Epoch 521/600 | Train Loss: 0.0045 | Val Loss: 0.0053\n",
      "Epoch 531/600 | Train Loss: 0.0044 | Val Loss: 0.0050\n",
      "Epoch 541/600 | Train Loss: 0.0043 | Val Loss: 0.0045\n",
      "Epoch 551/600 | Train Loss: 0.0044 | Val Loss: 0.0049\n",
      "Epoch 561/600 | Train Loss: 0.0042 | Val Loss: 0.0047\n",
      "Epoch 571/600 | Train Loss: 0.0044 | Val Loss: 0.0044\n",
      "Epoch 581/600 | Train Loss: 0.0042 | Val Loss: 0.0048\n",
      "Epoch 591/600 | Train Loss: 0.0043 | Val Loss: 0.0050\n",
      "Epoch 600/600 | Train Loss: 0.0043 | Val Loss: 0.0048\n",
      "Best validation loss with He Initialization and Compact Model: 0.0042 at epoch 562\n"
     ]
    }
   ],
   "source": [
    "he_compact = LinearlyActuatedStrutsHeCompact()\n",
    "\n",
    "he_compact_history = train_model(\n",
    "    model=he_compact,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    model_name=\"He_Model_LR_Compact\",\n",
    "    num_epochs=600,\n",
    "    scale_factor=73.8,  # Your specific scaling factor\n",
    "    lr= 1e-3,  # Learning rate for the second model\n",
    ")\n",
    "\n",
    "print(f\"Best validation loss with He Initialization and Compact Model: {he_compact_history['best_val_loss']:.4f} at epoch {he_compact_history['epoch_best']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: He_Model_LR_Compact | MAE: 0.3541\n",
      "tensor([[73.7000, 73.7000, 73.7000,  0.3000, 73.6000, 56.5000],\n",
      "        [12.7000, 73.8000, 73.3000, 73.0000, 73.3000, 73.8000],\n",
      "        [73.8000, 73.8000, 73.8000, 73.3000, 68.1000,  0.2000],\n",
      "        ...,\n",
      "        [73.8000, 73.6000, 73.8000, 35.9000,  0.0000,  0.1000],\n",
      "        [73.8000, 73.6000, 73.8000, 72.9000,  4.5000,  0.0000],\n",
      "        [73.8000, 73.5000, 73.8000, 62.6000,  0.0000,  0.0000]])\n"
     ]
    }
   ],
   "source": [
    "he_compact_pred, he_compact_mae = calculate_model_performance(\"He_Model_LR_Compact\")\n",
    "he_compact_pred = torch.round(he_compact_pred, decimals=1)\n",
    "print(he_compact_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500 | Train Loss: 2.4069 | Val Loss: 1.0775\n",
      "Epoch 11/500 | Train Loss: 0.0356 | Val Loss: 0.0314\n",
      "Epoch 21/500 | Train Loss: 0.0246 | Val Loss: 0.0333\n",
      "Epoch 31/500 | Train Loss: 0.0147 | Val Loss: 0.0142\n",
      "Epoch 41/500 | Train Loss: 0.0135 | Val Loss: 0.0101\n",
      "Epoch 51/500 | Train Loss: 0.0115 | Val Loss: 0.0127\n",
      "Epoch 61/500 | Train Loss: 0.0094 | Val Loss: 0.0094\n",
      "Epoch 71/500 | Train Loss: 0.0094 | Val Loss: 0.0091\n",
      "Epoch 81/500 | Train Loss: 0.0083 | Val Loss: 0.0100\n",
      "Epoch 91/500 | Train Loss: 0.0098 | Val Loss: 0.0083\n",
      "Epoch 101/500 | Train Loss: 0.0077 | Val Loss: 0.0082\n",
      "Epoch 111/500 | Train Loss: 0.0074 | Val Loss: 0.0071\n",
      "Epoch 121/500 | Train Loss: 0.0079 | Val Loss: 0.0066\n",
      "Epoch 131/500 | Train Loss: 0.0071 | Val Loss: 0.0066\n",
      "Epoch 141/500 | Train Loss: 0.0071 | Val Loss: 0.0067\n",
      "Epoch 151/500 | Train Loss: 0.0072 | Val Loss: 0.0067\n",
      "Epoch 161/500 | Train Loss: 0.0069 | Val Loss: 0.0082\n",
      "Epoch 171/500 | Train Loss: 0.0070 | Val Loss: 0.0068\n",
      "Epoch 181/500 | Train Loss: 0.0064 | Val Loss: 0.0073\n",
      "Epoch 191/500 | Train Loss: 0.0067 | Val Loss: 0.0067\n",
      "Epoch 201/500 | Train Loss: 0.0070 | Val Loss: 0.0070\n",
      "Epoch 211/500 | Train Loss: 0.0069 | Val Loss: 0.0071\n",
      "Epoch 221/500 | Train Loss: 0.0066 | Val Loss: 0.0072\n",
      "Epoch 231/500 | Train Loss: 0.0066 | Val Loss: 0.0067\n",
      "Epoch 241/500 | Train Loss: 0.0065 | Val Loss: 0.0065\n",
      "Epoch 251/500 | Train Loss: 0.0068 | Val Loss: 0.0067\n",
      "Epoch 261/500 | Train Loss: 0.0065 | Val Loss: 0.0080\n",
      "Epoch 271/500 | Train Loss: 0.0065 | Val Loss: 0.0063\n",
      "Epoch 281/500 | Train Loss: 0.0066 | Val Loss: 0.0071\n",
      "Epoch 291/500 | Train Loss: 0.0063 | Val Loss: 0.0070\n",
      "Epoch 301/500 | Train Loss: 0.0065 | Val Loss: 0.0082\n",
      "Epoch 311/500 | Train Loss: 0.0063 | Val Loss: 0.0064\n",
      "Epoch 321/500 | Train Loss: 0.0063 | Val Loss: 0.0057\n",
      "Epoch 331/500 | Train Loss: 0.0063 | Val Loss: 0.0086\n",
      "Epoch 341/500 | Train Loss: 0.0063 | Val Loss: 0.0065\n",
      "Epoch 351/500 | Train Loss: 0.0061 | Val Loss: 0.0065\n",
      "Epoch 361/500 | Train Loss: 0.0062 | Val Loss: 0.0064\n",
      "Epoch 371/500 | Train Loss: 0.0068 | Val Loss: 0.0066\n",
      "Epoch 381/500 | Train Loss: 0.0063 | Val Loss: 0.0062\n",
      "Epoch 391/500 | Train Loss: 0.0062 | Val Loss: 0.0069\n",
      "Epoch 401/500 | Train Loss: 0.0063 | Val Loss: 0.0064\n",
      "Epoch 411/500 | Train Loss: 0.0061 | Val Loss: 0.0070\n",
      "Epoch 421/500 | Train Loss: 0.0063 | Val Loss: 0.0072\n",
      "Epoch 431/500 | Train Loss: 0.0060 | Val Loss: 0.0060\n",
      "Epoch 441/500 | Train Loss: 0.0062 | Val Loss: 0.0064\n",
      "Epoch 451/500 | Train Loss: 0.0063 | Val Loss: 0.0060\n",
      "Epoch 461/500 | Train Loss: 0.0064 | Val Loss: 0.0066\n",
      "Epoch 471/500 | Train Loss: 0.0062 | Val Loss: 0.0066\n",
      "Epoch 481/500 | Train Loss: 0.0062 | Val Loss: 0.0079\n",
      "Epoch 491/500 | Train Loss: 0.0061 | Val Loss: 0.0060\n",
      "Epoch 500/500 | Train Loss: 0.0060 | Val Loss: 0.0062\n",
      "Best validation loss with He Initialization: 0.0056 at epoch 453\n"
     ]
    }
   ],
   "source": [
    "model_he = LinearlyActuatedStrutsHe()\n",
    "# Train and evaluate\n",
    "history_he = train_model(\n",
    "    model=model_he,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    model_name=\"He_Model_1e-3WeightDecay\",\n",
    "    num_epochs=500,\n",
    "    scale_factor=73.8,  # Your specific scaling factor\n",
    "    optimizer = torch.optim.Adam(model_he.parameters(), lr=1e-3, weight_decay=1e-6)\n",
    ")\n",
    "\n",
    "print(f\"Best validation loss with He Initialization: {history_he['best_val_loss']:.4f} at epoch {history_he['epoch_best']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: He_Model_1e-3 | MAE: 0.3057\n",
      "tensor([[73.8000, 73.8000, 73.8000,  0.1000, 73.5000, 56.7000],\n",
      "        [13.2000, 73.8000, 73.3000, 73.1000, 73.3000, 73.8000],\n",
      "        [73.6000, 73.7000, 73.7000, 73.2000, 68.0000,  0.0000],\n",
      "        ...,\n",
      "        [73.4000, 73.6000, 73.7000, 36.5000,  0.0000,  0.0000],\n",
      "        [73.6000, 73.6000, 73.7000, 72.8000,  4.1000,  0.0000],\n",
      "        [73.4000, 73.6000, 73.7000, 63.2000,  0.0000,  0.0000]])\n"
     ]
    }
   ],
   "source": [
    "model_he_pred, model_he_mae = calculate_model_performance(\"He_Model_1e-3\")\n",
    "model_he_pred = torch.round(model_he_pred, decimals=1)\n",
    "print(model_he_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200 | Train Loss: 2.2402 | Val Loss: 0.4957\n",
      "Epoch 11/200 | Train Loss: 0.4450 | Val Loss: 0.1344\n",
      "Epoch 21/200 | Train Loss: 0.3093 | Val Loss: 0.1196\n",
      "Epoch 31/200 | Train Loss: 0.2342 | Val Loss: 0.1356\n",
      "Epoch 41/200 | Train Loss: 0.2316 | Val Loss: 0.1168\n",
      "Epoch 51/200 | Train Loss: 0.1979 | Val Loss: 0.2031\n",
      "Epoch 61/200 | Train Loss: 0.1761 | Val Loss: 0.2867\n",
      "Epoch 71/200 | Train Loss: 0.1532 | Val Loss: 0.0731\n",
      "Epoch 81/200 | Train Loss: 0.1320 | Val Loss: 0.0805\n",
      "Epoch 91/200 | Train Loss: 0.1430 | Val Loss: 0.0684\n",
      "Epoch 101/200 | Train Loss: 0.1372 | Val Loss: 0.0913\n",
      "Epoch 111/200 | Train Loss: 0.1208 | Val Loss: 0.0796\n",
      "Epoch 121/200 | Train Loss: 0.1130 | Val Loss: 0.0580\n",
      "Epoch 131/200 | Train Loss: 0.1106 | Val Loss: 0.0803\n",
      "Epoch 141/200 | Train Loss: 0.1125 | Val Loss: 0.0894\n",
      "Epoch 151/200 | Train Loss: 0.1030 | Val Loss: 0.0905\n",
      "Epoch 161/200 | Train Loss: 0.1081 | Val Loss: 0.1344\n",
      "Epoch 171/200 | Train Loss: 0.1035 | Val Loss: 0.0920\n",
      "Epoch 181/200 | Train Loss: 0.0990 | Val Loss: 0.0975\n",
      "Epoch 191/200 | Train Loss: 0.1095 | Val Loss: 0.0847\n",
      "Epoch 200/200 | Train Loss: 0.1071 | Val Loss: 0.1375\n",
      "Best validation loss with Deep NN: 0.0547 at epoch 195\n"
     ]
    }
   ],
   "source": [
    "deep_nn = DeepLinearlyActuatedStruts()\n",
    "# Train and evaluate\n",
    "history_deep_nn = train_model(\n",
    "    model=deep_nn,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    model_name=\"Deep_NN_Model\",\n",
    "    num_epochs=200,\n",
    "    scale_factor=73.8,  # Your specific scaling factor\n",
    "    lr=1e-3,  # Learning rate for the deep model\n",
    ")\n",
    "\n",
    "print(f\"Best validation loss with Deep NN: {history_deep_nn['best_val_loss']:.4f} at epoch {history_deep_nn['epoch_best']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Deep_NN_Model | MAE: 1.7180\n",
      "tensor([[73.8000, 73.8000, 73.8000,  1.6000, 73.8000, 52.9000],\n",
      "        [17.9000, 73.7000, 73.5000, 73.7000, 73.8000, 73.8000],\n",
      "        [73.7000, 72.3000, 73.5000, 72.5000, 66.8000,  1.7000],\n",
      "        ...,\n",
      "        [73.8000, 73.8000, 73.8000, 34.6000,  0.2000,  1.0000],\n",
      "        [73.8000, 73.7000, 73.8000, 71.0000,  8.8000,  1.3000],\n",
      "        [73.8000, 73.8000, 73.8000, 60.0000,  1.1000,  1.5000]])\n"
     ]
    }
   ],
   "source": [
    "deep_nn_pred, deep_nn_mae = calculate_model_performance(\"Deep_NN_Model\")\n",
    "\n",
    "deep_nn_pred = torch.round(deep_nn_pred, decimals=1)\n",
    "print(deep_nn_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "He init 3 hidden layer 1e-3 and L1 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/700 | Train Loss: 3.9237 | Val Loss: 1.0925\n",
      "Epoch 11/700 | Train Loss: 0.5533 | Val Loss: 0.0824\n",
      "Epoch 21/700 | Train Loss: 0.3257 | Val Loss: 0.0518\n",
      "Epoch 31/700 | Train Loss: 0.2659 | Val Loss: 0.0419\n",
      "Epoch 41/700 | Train Loss: 0.2361 | Val Loss: 0.0387\n",
      "Epoch 51/700 | Train Loss: 0.2153 | Val Loss: 0.0373\n",
      "Epoch 61/700 | Train Loss: 0.2024 | Val Loss: 0.0371\n",
      "Epoch 71/700 | Train Loss: 0.1960 | Val Loss: 0.0359\n",
      "Epoch 81/700 | Train Loss: 0.1884 | Val Loss: 0.0365\n",
      "Epoch 91/700 | Train Loss: 0.1826 | Val Loss: 0.0334\n",
      "Epoch 101/700 | Train Loss: 0.1788 | Val Loss: 0.0352\n",
      "Epoch 111/700 | Train Loss: 0.1761 | Val Loss: 0.0360\n",
      "Epoch 121/700 | Train Loss: 0.1751 | Val Loss: 0.0349\n",
      "Epoch 131/700 | Train Loss: 0.1728 | Val Loss: 0.0320\n",
      "Epoch 141/700 | Train Loss: 0.1706 | Val Loss: 0.0319\n",
      "Epoch 151/700 | Train Loss: 0.1708 | Val Loss: 0.0326\n",
      "Epoch 161/700 | Train Loss: 0.1696 | Val Loss: 0.0334\n",
      "Epoch 171/700 | Train Loss: 0.1670 | Val Loss: 0.0320\n",
      "Epoch 181/700 | Train Loss: 0.1659 | Val Loss: 0.0332\n",
      "Epoch 191/700 | Train Loss: 0.1645 | Val Loss: 0.0305\n",
      "Epoch 201/700 | Train Loss: 0.1640 | Val Loss: 0.0307\n",
      "Epoch 211/700 | Train Loss: 0.1631 | Val Loss: 0.0349\n",
      "Epoch 221/700 | Train Loss: 0.1619 | Val Loss: 0.0303\n",
      "Epoch 231/700 | Train Loss: 0.1596 | Val Loss: 0.0304\n",
      "Epoch 241/700 | Train Loss: 0.1591 | Val Loss: 0.0313\n",
      "Epoch 251/700 | Train Loss: 0.1574 | Val Loss: 0.0312\n",
      "Epoch 261/700 | Train Loss: 0.1565 | Val Loss: 0.0294\n",
      "Epoch 271/700 | Train Loss: 0.1556 | Val Loss: 0.0308\n",
      "Epoch 281/700 | Train Loss: 0.1554 | Val Loss: 0.0299\n",
      "Epoch 291/700 | Train Loss: 0.1543 | Val Loss: 0.0302\n",
      "Epoch 301/700 | Train Loss: 0.1542 | Val Loss: 0.0297\n",
      "Epoch 311/700 | Train Loss: 0.1536 | Val Loss: 0.0307\n",
      "Epoch 321/700 | Train Loss: 0.1534 | Val Loss: 0.0316\n",
      "Epoch 331/700 | Train Loss: 0.1531 | Val Loss: 0.0322\n",
      "Epoch 341/700 | Train Loss: 0.1524 | Val Loss: 0.0294\n",
      "Epoch 351/700 | Train Loss: 0.1521 | Val Loss: 0.0294\n",
      "Epoch 361/700 | Train Loss: 0.1518 | Val Loss: 0.0299\n",
      "Epoch 371/700 | Train Loss: 0.1529 | Val Loss: 0.0310\n",
      "Epoch 381/700 | Train Loss: 0.1513 | Val Loss: 0.0287\n",
      "Epoch 391/700 | Train Loss: 0.1511 | Val Loss: 0.0317\n",
      "Epoch 401/700 | Train Loss: 0.1513 | Val Loss: 0.0306\n",
      "Epoch 411/700 | Train Loss: 0.1509 | Val Loss: 0.0290\n",
      "Epoch 421/700 | Train Loss: 0.1509 | Val Loss: 0.0298\n",
      "Epoch 431/700 | Train Loss: 0.1506 | Val Loss: 0.0301\n",
      "Epoch 441/700 | Train Loss: 0.1508 | Val Loss: 0.0326\n",
      "Epoch 451/700 | Train Loss: 0.1508 | Val Loss: 0.0299\n",
      "Epoch 461/700 | Train Loss: 0.1508 | Val Loss: 0.0304\n",
      "Epoch 471/700 | Train Loss: 0.1503 | Val Loss: 0.0301\n",
      "Epoch 481/700 | Train Loss: 0.1506 | Val Loss: 0.0296\n",
      "Epoch 491/700 | Train Loss: 0.1509 | Val Loss: 0.0305\n",
      "Epoch 501/700 | Train Loss: 0.1503 | Val Loss: 0.0296\n",
      "Epoch 511/700 | Train Loss: 0.1507 | Val Loss: 0.0311\n",
      "Epoch 521/700 | Train Loss: 0.1502 | Val Loss: 0.0295\n",
      "Epoch 531/700 | Train Loss: 0.1502 | Val Loss: 0.0289\n",
      "Epoch 541/700 | Train Loss: 0.1498 | Val Loss: 0.0300\n",
      "Epoch 551/700 | Train Loss: 0.1500 | Val Loss: 0.0299\n",
      "Epoch 561/700 | Train Loss: 0.1508 | Val Loss: 0.0313\n",
      "Epoch 571/700 | Train Loss: 0.1501 | Val Loss: 0.0299\n",
      "Epoch 581/700 | Train Loss: 0.1503 | Val Loss: 0.0333\n",
      "Epoch 591/700 | Train Loss: 0.1500 | Val Loss: 0.0297\n",
      "Epoch 601/700 | Train Loss: 0.1503 | Val Loss: 0.0290\n",
      "Epoch 611/700 | Train Loss: 0.1499 | Val Loss: 0.0301\n",
      "Epoch 621/700 | Train Loss: 0.1500 | Val Loss: 0.0288\n",
      "Epoch 631/700 | Train Loss: 0.1501 | Val Loss: 0.0294\n",
      "Epoch 641/700 | Train Loss: 0.1510 | Val Loss: 0.0291\n",
      "Epoch 651/700 | Train Loss: 0.1503 | Val Loss: 0.0305\n",
      "Epoch 661/700 | Train Loss: 0.1497 | Val Loss: 0.0299\n",
      "Epoch 671/700 | Train Loss: 0.1499 | Val Loss: 0.0314\n",
      "Epoch 681/700 | Train Loss: 0.1497 | Val Loss: 0.0305\n",
      "Epoch 691/700 | Train Loss: 0.1504 | Val Loss: 0.0311\n",
      "Epoch 700/700 | Train Loss: 0.1501 | Val Loss: 0.0312\n",
      "Best validation loss with He Initialization and L1 regularization: 0.0281 at epoch 336\n"
     ]
    }
   ],
   "source": [
    "he_l1 = LinearlyActuatedStrutsHe()\n",
    "# Train and evaluate with L1 regularization\n",
    "history_he_l1 = train_model(\n",
    "    model=he_l1,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    model_name=\"He_Model_L1\",\n",
    "    num_epochs=700,\n",
    "    scale_factor=73.8,  # Your specific scaling factor\n",
    "    l1_lambda=1e-5,  # L1 regularization strength\n",
    ")\n",
    "print(f\"Best validation loss with He Initialization and L1 regularization: {history_he_l1['best_val_loss']:.4f} at epoch {history_he_l1['epoch_best']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
