{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    \"\"\"Set all random seeds for reproducibility\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  \n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "set_seed(42)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>Z</th>\n",
       "      <th>A0</th>\n",
       "      <th>A1</th>\n",
       "      <th>A2</th>\n",
       "      <th>A5</th>\n",
       "      <th>A6</th>\n",
       "      <th>A7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.759297</td>\n",
       "      <td>0.419074</td>\n",
       "      <td>3.31</td>\n",
       "      <td>2.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.94</td>\n",
       "      <td>1.58</td>\n",
       "      <td>1.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.058824</td>\n",
       "      <td>-0.759205</td>\n",
       "      <td>0.418863</td>\n",
       "      <td>3.39</td>\n",
       "      <td>2.23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.09</td>\n",
       "      <td>1.73</td>\n",
       "      <td>1.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.117647</td>\n",
       "      <td>-0.759242</td>\n",
       "      <td>0.419042</td>\n",
       "      <td>3.46</td>\n",
       "      <td>2.16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.09</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.176471</td>\n",
       "      <td>-0.759302</td>\n",
       "      <td>0.419248</td>\n",
       "      <td>3.67</td>\n",
       "      <td>2.30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.23</td>\n",
       "      <td>1.87</td>\n",
       "      <td>1.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.235294</td>\n",
       "      <td>-0.759177</td>\n",
       "      <td>0.418970</td>\n",
       "      <td>3.82</td>\n",
       "      <td>2.30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.16</td>\n",
       "      <td>1.87</td>\n",
       "      <td>1.94</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          X         Y         Z    A0    A1   A2    A5    A6    A7\n",
       "0  0.000000 -0.759297  0.419074  3.31  2.01  0.0  1.94  1.58  1.65\n",
       "1  0.058824 -0.759205  0.418863  3.39  2.23  0.0  2.09  1.73  1.80\n",
       "2  0.117647 -0.759242  0.419042  3.46  2.16  0.0  2.09  1.80  1.87\n",
       "3  0.176471 -0.759302  0.419248  3.67  2.30  0.0  2.23  1.87  1.94\n",
       "4  0.235294 -0.759177  0.418970  3.82  2.30  0.0  2.16  1.87  1.94"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('dataset.csv')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10708 entries, 0 to 10707\n",
      "Data columns (total 9 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   X       10708 non-null  float64\n",
      " 1   Y       10708 non-null  float64\n",
      " 2   Z       10708 non-null  float64\n",
      " 3   A0      10708 non-null  float64\n",
      " 4   A1      10708 non-null  float64\n",
      " 5   A2      10708 non-null  float64\n",
      " 6   A5      10708 non-null  float64\n",
      " 7   A6      10708 non-null  float64\n",
      " 8   A7      10708 non-null  float64\n",
      "dtypes: float64(9)\n",
      "memory usage: 753.0 KB\n"
     ]
    }
   ],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = dataset[[\"X\", \"Y\", \"Z\"]], dataset[['A0', 'A1', 'A2', 'A5', 'A6', 'A7']]\n",
    "\n",
    "x_mean = X.iloc[:, 0].mean()\n",
    "\n",
    "x_std = X.iloc[:, 0].std()\n",
    "y_mean = X.iloc[:, 1].mean()\n",
    "y_std = X.iloc[:, 1].std()\n",
    "z_mean = X.iloc[:, 2].mean()\n",
    "z_std = X.iloc[:, 2].std()\n",
    "\n",
    "# Normalize each column\n",
    "X_norm = np.copy(X)\n",
    "X_norm[:, 0] = (X.iloc[:, 0] - x_mean) / x_std\n",
    "X_norm[:, 1] = (X.iloc[:, 1] - y_mean) / y_std\n",
    "X_norm[:, 2] = (X.iloc[:, 2] - z_mean) / z_std\n",
    "\n",
    "y_norm = y / 73.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_norm = torch.tensor(pd.DataFrame(X_norm).values, dtype=torch.float32)\n",
    "y_norm = torch.tensor(pd.DataFrame(y_norm).values, dtype=torch.float32)\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X_norm, y_norm, test_size=0.2, shuffle=True, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9425,  0.6090,  1.6928],\n",
       "        [-0.7765,  1.6047,  0.8203],\n",
       "        [ 1.5648, -0.5537,  0.8535],\n",
       "        ...,\n",
       "        [-0.1568, -0.2147, -1.0535],\n",
       "        [-1.2321, -1.6867,  0.6647],\n",
       "        [ 0.6153,  0.3961,  0.7643]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "\n",
    "# Create loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearlyActuatedStrutsXavier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearlyActuatedStrutsXavier, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(3, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 6),\n",
    "            nn.Sigmoid()  # Constrain to [0, 1]\n",
    "        )\n",
    "        \n",
    "        # Initialize weights with Xavier/Glorot initialization\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.model.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                torch.manual_seed(42) # Set seed for reproducibility\n",
    "                init.xavier_uniform_(m.weight)  # Xavier uniform initialization\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)      # Initialize biases to zero\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "\n",
    "class LinearlyActuatedStrutsHe(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearlyActuatedStrutsHe, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(3, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 6),\n",
    "            nn.Sigmoid()  # Constrain to [0, 1]\n",
    "        )\n",
    "        \n",
    "        # Initialize weights with Xavier/Glorot initialization\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.model.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                torch.manual_seed(42) # Set seed for reproducibility\n",
    "                init.kaiming_uniform_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    model_name: str = \"model\",\n",
    "    criterion: nn.Module = nn.MSELoss(),\n",
    "    optimizer: torch.optim.Optimizer = None,\n",
    "    lr: float = 1e-3,\n",
    "    num_epochs: int = 100,\n",
    "    scale_factor: float = 73.8,  \n",
    "    save_best: bool = True,\n",
    "    verbose: bool = True\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Train and evaluate a single PyTorch model\n",
    "    \n",
    "    Args:\n",
    "        model: Initialized PyTorch model\n",
    "        train_loader: Training data loader\n",
    "        val_loader: Validation data loader\n",
    "        model_name: Name for saving/printing\n",
    "        criterion: Loss function (default MSELoss)\n",
    "        optimizer: Optimizer (defaults to Adam if None)\n",
    "        lr: Learning rate if optimizer is None\n",
    "        num_epochs: Training epochs\n",
    "        scale_factor: Loss scaling factor\n",
    "        save_best: Whether to save best model weights\n",
    "        verbose: Print progress\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with training history and best model state\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    if optimizer is None:\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'best_val_loss': float('inf'),\n",
    "        'best_model_state': None,\n",
    "        'epoch_best': 0\n",
    "    }\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        # Validation\n",
    "        val_loss = 0.0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        # Record metrics\n",
    "        train_loss = train_loss / len(train_loader.dataset) * scale_factor\n",
    "        val_loss = val_loss / len(val_loader.dataset) * scale_factor\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        \n",
    "        # Update best model\n",
    "        if val_loss < history['best_val_loss']:\n",
    "            history['best_val_loss'] = val_loss\n",
    "            history['best_model_state'] = model.state_dict()\n",
    "            history['epoch_best'] = epoch + 1\n",
    "        \n",
    "        # Print progress\n",
    "        if verbose and (epoch % 10 == 0 or epoch == num_epochs - 1):\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs} | \"\n",
    "                  f\"Train Loss: {train_loss:.4f} | \"\n",
    "                  f\"Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    # Save best model weights\n",
    "    if save_best and history['best_model_state'] is not None:\n",
    "        torch.save(history['best_model_state'], f'best_{model_name}.pth')\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300 | Train Loss: 4.8894 | Val Loss: 1.8403\n",
      "Epoch 11/300 | Train Loss: 0.1250 | Val Loss: 0.1478\n",
      "Epoch 21/300 | Train Loss: 0.0555 | Val Loss: 0.0948\n",
      "Epoch 31/300 | Train Loss: 0.0448 | Val Loss: 0.0380\n",
      "Epoch 41/300 | Train Loss: 0.0385 | Val Loss: 0.0356\n",
      "Epoch 51/300 | Train Loss: 0.0313 | Val Loss: 0.0350\n",
      "Epoch 61/300 | Train Loss: 0.0355 | Val Loss: 0.0408\n",
      "Epoch 71/300 | Train Loss: 0.0293 | Val Loss: 0.0227\n",
      "Epoch 81/300 | Train Loss: 0.0297 | Val Loss: 0.0244\n",
      "Epoch 91/300 | Train Loss: 0.0244 | Val Loss: 0.0258\n",
      "Epoch 101/300 | Train Loss: 0.0199 | Val Loss: 0.0286\n",
      "Epoch 111/300 | Train Loss: 0.0189 | Val Loss: 0.0143\n",
      "Epoch 121/300 | Train Loss: 0.0188 | Val Loss: 0.0220\n",
      "Epoch 131/300 | Train Loss: 0.0189 | Val Loss: 0.0156\n",
      "Epoch 141/300 | Train Loss: 0.0158 | Val Loss: 0.0149\n",
      "Epoch 151/300 | Train Loss: 0.0159 | Val Loss: 0.0175\n",
      "Epoch 161/300 | Train Loss: 0.0147 | Val Loss: 0.0206\n",
      "Epoch 171/300 | Train Loss: 0.0159 | Val Loss: 0.0144\n",
      "Epoch 181/300 | Train Loss: 0.0162 | Val Loss: 0.0213\n",
      "Epoch 191/300 | Train Loss: 0.0138 | Val Loss: 0.0104\n",
      "Epoch 201/300 | Train Loss: 0.0263 | Val Loss: 0.0133\n",
      "Epoch 211/300 | Train Loss: 0.0121 | Val Loss: 0.0119\n",
      "Epoch 221/300 | Train Loss: 0.0134 | Val Loss: 0.0105\n",
      "Epoch 231/300 | Train Loss: 0.0126 | Val Loss: 0.0108\n",
      "Epoch 241/300 | Train Loss: 0.0121 | Val Loss: 0.0117\n",
      "Epoch 251/300 | Train Loss: 0.0125 | Val Loss: 0.0100\n",
      "Epoch 261/300 | Train Loss: 0.0136 | Val Loss: 0.0177\n",
      "Epoch 271/300 | Train Loss: 0.0142 | Val Loss: 0.0125\n",
      "Epoch 281/300 | Train Loss: 0.0112 | Val Loss: 0.0121\n",
      "Epoch 291/300 | Train Loss: 0.0102 | Val Loss: 0.0327\n",
      "Epoch 300/300 | Train Loss: 0.0111 | Val Loss: 0.0116\n",
      "Best validation loss with Xavier Initialization: 0.0081 at epoch 279\n"
     ]
    }
   ],
   "source": [
    "# Initialize your model\n",
    "model_xavier = LinearlyActuatedStrutsXavier()\n",
    "\n",
    "# Train and evaluate\n",
    "history_xavier = train_model(\n",
    "    model=model_xavier,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    model_name=\"Xavier_Model_1e-3\",\n",
    "    num_epochs=300,\n",
    "    scale_factor=73.8  # Your specific scaling factor\n",
    ")\n",
    "\n",
    "# Access results\n",
    "print(f\"Best validation loss with Xavier Initialization: {history_xavier['best_val_loss']:.4f} at epoch {history_xavier['epoch_best']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0EAAAHWCAYAAACxAYILAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8ekN5oAAAACXBIWXMAAA9hAAAPYQGoP6dpAABY1klEQVR4nO3dB5hU5dn/8d/07buw9A6CKKio2LArdmOJGo0posmrr7FEX6MxxlgTo4kmfxNNTNfExB5bjBV7Q0BEVBQFkQ4LLNvblPO/7gdm3KUo6Axzdvf7ua5hdmdmZ56Zs7PM79z385yA53meAAAAAKCbCOZ7AAAAAACwNRGCAAAAAHQrhCAAAAAA3QohCAAAAEC3QggCAAAA0K0QggAAAAB0K4QgAAAAAN0KIQgAAABAt0IIAgAAANCtEIIAAOjEPvnkEwUCAd100035HgoAdBqEIADwgTvuuMN9kJ0+fXq+h4JNhIxNnW644YZ8DxEAsIXCW/oDAAB0R6eeeqqOOuqoDS7fZZdd8jIeAMAXRwgCAHR7jY2NKi4u/szb7LrrrvrWt7611cYEAMgd2uEAoBN56623dOSRR6qsrEwlJSWaOHGipkyZ0uE28Xhc11xzjUaNGqWCggJVVlZq33331TPPPJO5zfLly3XGGWdo0KBBisVi6t+/v4477jjX+tXeE088of32288FhNLSUh199NF67733Otxmc+9rY5577rnM/VdUVLife//99zPXP/DAA67l7MUXX9zgZ//4xz+66959993MZR988IFOOukk9ezZ0z333XbbTY8++uhGWw/tPs855xz16dPHjT0bhg0bpq985St6+umntfPOO7sxjBkzRg8++OAGt/3444/1ta99zY21qKhIe+21l/773/9ucLuWlhZdffXV2nbbbd392et7wgknaN68eRvc9k9/+pO22WYbtx123313TZs2LWvbCgC6EipBANBJWPiwwGAB6Ic//KEikYgLAgceeKD7QL/nnnu629kH5uuvv17/8z//oz322EN1dXVurtGMGTN06KGHutuceOKJ7v7OP/9898G9qqrKhaSFCxe6782dd96pSZMm6fDDD9cvfvELNTU16bbbbnOBysJY+nabc18bM3nyZBfoRowY4cbc3NysW265Rfvss48bq/2shS4Le/fdd58OOOCADj9/7733auzYsdphhx0yr4/97MCBA/WjH/3IBSv7ueOPP17//ve/9dWvfrXDz1sA6t27t6688kpXCfo89vxXrVq1weUW3sLhT/87/eijj3TKKafo7LPPdq/f7bff7sLOk08+mXn9V6xYob333tvd5/e//30XVP/+97/r2GOPdcEvPdZkMulC1bPPPquvf/3ruuCCC1RfX+9eXwt/FnjS7rrrLnfd//7v/7qQ98tf/tKFJQtb9rvyZbYVAHQ5HgAg726//XbP/iRPmzZtk7c5/vjjvWg06s2bNy9z2dKlS73S0lJv//33z1w2btw47+ijj97k/axZs8Y91o033rjJ29TX13sVFRXemWee2eHy5cuXe+Xl5ZnLN+e+NmXnnXf2+vTp461evTpz2dtvv+0Fg0HvtNNOy1x26qmnutslEonMZcuWLXO3u/baazOXTZw40dtxxx29lpaWzGWpVMrbe++9vVGjRm3wWu+7774d7nNT5s+f726/qdPrr7+eue3QoUPdZf/+978zl9XW1nr9+/f3dtlll8xlF154obvdyy+/3OE1Hz58uDds2DAvmUy6y/72t7+52/3617/eYFz23NqPr7Ky0quurs5c/8gjj7jL//Of/3zpbQUAXQ3tcADQCVhFwFqsrKphlZM0a2f6xje+oVdeecVVfNKVCdvbbxWJjSksLFQ0GtULL7ygNWvWbPQ2Vh2oqalxiwFY9SN9CoVCruL0/PPPb/Z9bcyyZcs0c+ZMnX766a4dLG2nnXZy1ZLHH388c5lVVaxiYY+RZtWSVCrlrjPV1dWute7kk0921ZD0eFevXu0qWfZaLFmypMMYzjzzTPd8NtdZZ53lXpf1T9bu1t6AAQM6VJ2scnfaaae56pm1oxl7flals6pamlW87DGsNW327NnuMqtg9erVy1Vu1mfVnvbstejRo0fme6saGqsEfZltBQBdESEIADqBlStXutap0aNHb3Dd9ttv7wLBokWL3PfXXnutCzA2h2THHXfUJZdcolmzZmVub3NBrL3N5vv07dtX+++/v2udSn9AN+kAdfDBB7uWsfYnC2MWSjb3vjZmwYIF7nxTz8cCTLpF7YgjjlB5eblrf0uzr23OjT1HM3fuXOts0BVXXLHBeK+66ip3m/SY04YPH64tYXOsDjnkkA1OFnLaGzly5AYBJT3O9Nwbe/6beu7tXx+b92O3a99utylDhgzp8H06EKUDzxfdVgDQFRGCAKCLsQ+39uH5b3/7m5sv85e//MWtbGbnaRdeeKE+/PBDN3fIJttbeLAP4FatMBaq0vOCNlb9eOSRRzb7vr4s+/BuFbCHHnpIiUTCVXReffXVTBWo/XgvvvjijY7XThZO2rPKSFeyqaqWhcOtta0AoLNgYQQA6ASsomEriM2ZM2eD62xFtGAwqMGDB2cusxYzWwXMTg0NDS4Y2eIDtlhCmk2q/8EPfuBOVvmxysqvfvUr/fOf/8xMuLeV06za8Xk+6742ZujQoe58U8/HWsDaL1ltgccWDrAFAmz1OPtg3z4EpVsEbQGAzRlvLqWrUu2rQRY8THrxAXv+m3ru6evTr+sbb7zhVvxLL27wZW3ptgKArohKEAB0kr38hx12mKvAtF/O2FYZs1XBbG5Jui3L5sG0Z3NNrArS2trqvre2Olt2ef0PxrYEdvo2No/G7u/nP/+5+wC+sfa8zb2vjbG5TPbh24KNte6l2Ypn1m63/kFJLdhYsLM2ODvZfJr27WwW1myVPFstz+YbbWq8W8PSpUtd1SrN5mr94x//cM+3X79+7jJ7flOnTtXrr7+euZ21/9kS1xaU0vOMbDU3aw289dZbP7PCszm+6LYCgK6IShAA+Ii1sNlSyuuzpZF/9rOfubYuCzy2vLPNE7EP/fYB1uZ2pNkHaAsE48ePd8HBlse2hQTOO++8TFXCji9kiwjYbe1+7EO7BSpbhtlYALLlsL/97W+7Vjq73KpRtpSyHcvGlqK2D+abc1+bcuONN7olsidMmKDvfve7mSWybf6PVa3asyqILfd8zz33uLBw0003bXB/v/vd79xrY/OgbNEDqw7ZOCxoLF68WG+//ba+DFu2e2PVEgsS9hzaz/+x52PH6LG5N7ZNbRy2VHaaLeF99913u+dvS2TbdrJAOH/+fLcYglX2jC2oYAHqoosucqHJFjuw52/Li9vvgB3jZ3N9mW0FAF1OvpenAwB8umzzpk6LFi1yt5sxY4Z3+OGHeyUlJV5RUZF30EEHea+99lqH+/rZz37m7bHHHm6J68LCQm+77bbzrrvuOq+trc1dv2rVKu/cc891lxcXF7slr/fcc0/vvvvu22Bczz//vHs8u01BQYG3zTbbeKeffro3ffr0Lb6vjZk8ebK3zz77uHGWlZV5xxxzjDd79uyN3vaZZ55xr0UgEMi8Huuz5cNtee1+/fp5kUjEGzhwoPeVr3zFe+CBB7ZoOfItWSJ70qRJHZbItuXJn3rqKW+nnXbyYrGYe23uv//+jY71pJNOctvJXlvbZo899tgGt2tqavIuv/xyt3y2PSd7bvZz6aXS0+Pb2NLXdvlVV12VlW0FAF1JwP7JdxADAKArsFY2W4zisccey/dQAACfgTlBAAAAALoVQhAAAACAboUQBAAAAKBbYU4QAAAAgG6FShAAAACAboUQBAAAAKBb6dQHS02lUu7I3Ha060AgkO/hAAAAAMgTm+VTX1+vAQMGZA463SVDkAWgwYMH53sYAAAAAHxi0aJFGjRoUNcNQVYBSj/RsrKyfA8HAAAAQJ7U1dW5Akk6I3TZEJRugbMARAgCAAAAENiMaTIsjAAAAACgWyEEAQAAAOhW8hqCrr76aleuan/abrvt8jkkAAAAAF1c3ucEjR07VpMnT858Hw7nfUgAAAD4EpLJpOLxeL6HgS4mFAq5rJCNQ+PkPXHYE+nXr1++hwEAAIAsaGho0OLFi90xW4BsKyoqUv/+/RWNRjt3CProo4/cAY0KCgo0YcIEXX/99RoyZMhGb9va2upO7ZfBAwAAgH8qQBaA7INq7969OZg9ssZCdVtbm1auXKn58+dr1KhRn3tAVN+GoD333FN33HGHRo8erWXLlumaa67Rfvvtp3fffXej63tbQLLbAAAAwH+sBc4+rFoAKiwszPdw0MUUFhYqEolowYIFLhBZEeWLCng+qlXW1NRo6NCh+vWvf63vfve7m1UJsgMi1dbWcpwgAACAPGtpaXF76YcPH/6lPqACX+R3zLJBeXn5ZmWDvLfDtVdRUaFtt91Wc+fO3ej1sVjMnQAAAACgSxwnyCbSzZs3z012AgAAAIAuF4Iuvvhivfjii/rkk0/02muv6atf/apb+u7UU0/N57AAAACAL2XYsGG6+eab8z0M+DEE2eohFnhsYYSTTz5ZlZWVmjJliptMBwAAAOSarWD3Waerr776C93vtGnTdNZZZ32psR144IG68MILv9R9wIdzgu655558PjwAAAC6OVuhOO3ee+/VlVdeqTlz5mQuKykpyXxt64nZMuB2nMvPw059f/PVnKDO7Lr/ztbh/+8lPTJzSb6HAgAA4AsWGpraEnk5be4CyP369cucbGUxq/6kv//ggw/cYVueeOIJjR8/3i3Q9corr7g57Mcdd5z69u3rQtLuu++uyZMnf2Y7nN3vX/7yFzf9w46jZMe5efTRR7/U6/vvf/9bY8eOdeOyx/vVr37V4frf//737nFsFTUb60knnZS57oEHHtCOO+7olp22bqxDDjlEjY2N6i58tTpcZ7a0pkVzVtRrTWNbvocCAADgC83xpMZc+VReHnv2tYerKJqdj7o/+tGPdNNNN2nEiBHq0aOHFi1apKOOOkrXXXedCyD/+Mc/dMwxx7gK0pAhQzZ5P3a8y1/+8pe68cYbdcstt+ib3/ymO+ZNz549t3hMb775pptOYu16p5xyiptff84557hAc/rpp2v69On6/ve/rzvvvFN77723qqur9fLLL2eqX6eeeqobi4Wy+vp6d52PjpyTc4SgLAkG1x4ROdV9fncAAAC6hWuvvVaHHnpo5nsLLePGjct8/9Of/lQPPfSQq+ycd955m7wfCyfpBcB+/vOf67e//a2mTp2qI444YovHZMfVnDhxoq644gr3vR1mZvbs2S5g2eMsXLhQxcXF+spXvuKqWXYszl122SUTghKJhE444QR3ubGqUHdCCMqSdRlIqW6UoAEAAD5LYSTkKjL5euxs2W233TY4rItVYP773/9mAkVzc7MLHp9lp512ynxtAcUO6FlVVfWFxvT++++7lrz29tlnH9eCZ/OWLLRZwLHqlYUsO6Vb8caNG+cClAWfww8/XIcddphrlbMqV3fBnKAsCQbSlSBCEAAAQHoejLWk5eNkj50tFljWP8yLVX6smmNtZDNnznSBoq3ts6dFRCKRDV6fVCqlXLDqz4wZM3T33Xe7Y3Dagg8WfmpqatwhaZ555hk312nMmDGuNc9Wa54/f766C0JQ1kNQvkcCAACAXHr11Vddy5lVViz82CIKdtzLrWn77bd341h/XNYWZyHH2Cp2tuCBzf2ZNWuWG+Nzzz2XCWBWObJ5Sm+99Zai0agLdt0F7XBZQjscAABA92Arrj344INuMQQLEzYvJ1cVnZUrV7pKU3tW2fnBD37gVqWz+Ui2MMLrr7+uW2+91a0IZx577DF9/PHH2n///V2b2+OPP+7GaBWfN954Q88++6xrg+vTp4/73h7HglV3QQjKciWIDAQAANC12aIE3/nOd9yqa7169dKll16qurq6nDzWXXfd5U7tWfD5yU9+ovvuu8+1udn3FoxsAQerUJmKigoX1GzuUktLiwtu1hpnS2q///77eumll9z8IRu3zR2y5bWPPPJIdRcBrxOvhWcbzdZzr62tdRPL8umyB2fp7qmLdNGh2+r7E0fldSwAAAD5YB+2bV7J8OHD3bFpgK35O7Yl2YA5QVnCwggAAABA50AIyhIWRgAAAAA6B0JQthdGIAUBAAAAvkYIypLguhREOxwAAADgb4SgLKEdDgAAAOgcCEFZwnGCAAAAgM6BEJTtdjhKQQAAAICvEYKyhHY4AAAAoHMgBGUJ7XAAAABA50AIyhIOlgoAANB9HXjggbrwwgsz3w8bNkw333zzZ/5MIBDQww8//KUfO1v3050QgrKEEAQAAND5HHPMMTriiCM2et3LL7/sAsasWbO2+H6nTZums846S9l09dVXa+edd97g8mXLlunII49ULt1xxx2qqKhQV0EIyhLmBAEAAHQ+3/3ud/XMM89o8eLFG1x3++23a7fddtNOO+20xffbu3dvFRUVaWvo16+fYrHYVnmsroIQlO05QaQgAACAtaxDpq0xP6fN7M75yle+4gKLVTraa2ho0P333+9C0urVq3Xqqadq4MCBLtjsuOOOuvvuuz/zftdvh/voo4+0//77q6CgQGPGjHHBa32XXnqptt12W/cYI0aM0BVXXKF4PO6us/Fdc801evvtt111yk7pMa/fDvfOO+/o4IMPVmFhoSorK11Fyp5P2umnn67jjz9eN910k/r37+9uc+6552Ye64tYuHChjjvuOJWUlKisrEwnn3yyVqxYkbnexn3QQQeptLTUXT9+/HhNnz7dXbdgwQJXkevRo4eKi4s1duxYPf7448qlcE7vvTsukU07HAAAwFrxJunnA/Lz2D9eKkWLP/dm4XBYp512mgsUl19+uQsUxgJQMpl04ccChH1ot5BiH+D/+9//6tvf/ra22WYb7bHHHp/7GKlUSieccIL69u2rN954Q7W1tR3mD6VZQLBxDBgwwAWZM8880132wx/+UKeccoreffddPfnkk5o8ebK7fXl5+Qb30djYqMMPP1wTJkxwLXlVVVX6n//5H5133nkdgt7zzz/vApCdz507192/tdrZY24pe37pAPTiiy8qkUi4UGX3+cILL7jbfPOb39Quu+yi2267TaFQSDNnzlQkEnHX2W3b2tr00ksvuRA0e/Zsd1+5RAjKEtrhAAAAOqfvfOc7uvHGG90HeFvgIN0Kd+KJJ7qgYaeLL744c/vzzz9fTz31lO67777NCkEWWj744AP3MxZwzM9//vMN5vH85Cc/6VBJsse85557XAiyqo4FAwtt1v62KXfddZdaWlr0j3/8wwUKc+utt7pKyy9+8QsXxIxVXexyCyTbbbedjj76aD377LNfKATZz1lomz9/vgYPHuwus8e3io4Fsd13391Vii655BL3WGbUqFGZn7fr7LW2CpuxKliuEYKyhHY4AACA9USK1lZk8vXYm8k+mO+9997629/+5kKQVUZsUYRrr73WXW8VIQstFnqWLFniqhatra2bPefn/fffd+EgHYCMVWrWd++99+q3v/2t5s2b56pPVlGxytOWsMcaN25cJgCZffbZx1Vr5syZkwlBY8eOdQEozapCFmS+iPTzSwcgYy1/tpCCXWch6KKLLnIVqTvvvFOHHHKIvva1r7lKmvn+97+v733ve3r66afddRaIvsg8rC3BnKAsCdEOBwAA0JF1ylhLWj5O67p0NpfN/fn3v/+t+vp6VwWyD+gHHHCAu86qRL/5zW9cO5y1j1krl7WcWRjKltdff921jB111FF67LHH9NZbb7n2vGw+RnuRda1oadYGaEEpV2xlu/fee89VnJ577jkXkh566CF3nYWjjz/+2LUYWhCzxShuueUW5RIhKEvS/aMUggAAADofm8gfDAZdO5m1clmLXPrz3auvvurmvHzrW99yVRZr1/rwww83+7633357LVq0yC1lnTZlypQOt3nttdc0dOhQF3wsBFi7mC0Y0F40GnVVqc97LFuEwOYGpdn47bmNHj1aubD9uudnpzSb11NTU+PCTpot+vB///d/ruJjc6QsbKZZFenss8/Wgw8+qB/84Af685//rFwiBGW7HY5KEAAAQKdj821sIv9ll13mwoqtoJZmgcRWc7OgYu1d//u//9th5bPPYy1eFgAmTZrkAoq12lnYac8ew+bG2Bwga4eztrh0paT9PCGbd2OVqFWrVrmWvPVZNclWoLPHsoUUrHJlc5isypJuhfuiLIDZY7c/2ethz8/m89hjz5gxQ1OnTnWLTVglzQJdc3OzW5jBFkmwYGehzOYKWXgytkiEzZey52Y/b2NOX5crhKAs4WCpAAAAnZu1xK1Zs8a1urWfv2MLFuy6667ucpszZAsT2BLTm8uqMBZoLAzYQgrW/nXdddd1uM2xxx7rqiQWFmyVNgtctkR2ezZXxg7saktN27LeG1um2+YpWaCorq52c3FOOukkTZw40S2C8GU1NDS4Fd7an2zBBauYPfLII26xBVsG3EKRVctsjpOxuUe2zLgFIwuDVnWzRSFsye90uLIV4iz42POz2/z+979XLgU8r/N+aq+rq3Orddgyg1s6aSzb7pyyQFc8/K6OGNtPf/j2+LyOBQAAIB9sVTLbmz98+HBXjQC25u/YlmQDKkFZQjscAAAA0DkQgrKEdjgAAACgcyAEZUmI1eEAAACAToEQlCXppeipBAEAAAD+RgjKcjtcklIQAADo5jrxulvoJr9bhKAsCa1bGYH3PAAA6K5sKWTT1taW76Ggi2pqanLnkUjkS91POEvj6fZohwMAAN1dOBx2x6lZuXKl+5Bqx8cBslUBsgBUVVWlioqKTOD+oghBWcLqcAAAoLuzg2b279/fHcdlwYIF+R4OuqCKigp3sNovixCU7RCUyvdIAAAA8icajWrUqFG0xCHrrLr4ZStAaYSgLAmtq/ZSCQIAAN2dtcEVFBTkexjAJtGomcXyryEEAQAAAP5GCMr2EtlkIAAAAMDXCEFZbodjXXwAAADA3whBWUI7HAAAANA5EIKyhNXhAAAAgM6BEJQlISpBAAAAQKdACMqS4NoMRAgCAAAAfI4QlPU5QfkeCQAAAIDPQgjKdiWIFAQAAAD4GiEoS0LrUhDtcAAAAIC/EYKyhHY4AAAAoHMgBGW5HS5JCgIAAAB8jRCU5XY4j3Y4AAAAwNcIQdk+WCoZCAAAAPA1QlCWrMtALIwAAAAA+BwhKEtYHQ4AAADoHAhBWUI7HAAAANA5EIKyfbBUKkEAAACArxGCslwJYolsAAAAwN8IQVkOQRSCAAAAAH8jBGV9ThApCAAAAPAz34SgG264QYFAQBdeeKE68xLZtMMBAAAA/uaLEDRt2jT98Y9/1E477aTOvkQ2hSAAAADA3/IeghoaGvTNb35Tf/7zn9WjRw91VrTDAQAAAJ1D3kPQueeeq6OPPlqHHHLI5962tbVVdXV1HU5+wRLZAAAAQOcQzueD33PPPZoxY4Zrh9sc119/va655hr5UXBdCrIpQZ7nuflNAAAAAPwnb5WgRYsW6YILLtC//vUvFRQUbNbPXHbZZaqtrc2c7D781g5nKAYBAAAA/pW3StCbb76pqqoq7brrrpnLksmkXnrpJd16662u9S0UCnX4mVgs5k5+lG6HS7fEBUUlCAAAAPCjvIWgiRMn6p133ulw2RlnnKHttttOl1566QYByO/S7XAm6Xn57TMEAAAAsEl5+6xeWlqqHXbYocNlxcXFqqys3ODyzoB2OAAAAKBzyPvqcF3F+u1wAAAAAPzJV11bL7zwgjqr9pWgpC0RBwAAAMCXqATlIASRgQAAAAD/IgTloB3OjhMEAAAAwJ8IQVlCJQgAAADoHAhBuVgimxQEAAAA+BYhKIvSOYh2OAAAAMC/CEE5aImjEAQAAAD4FyEoBy1xSSpBAAAAgG8RgnLQDpeiFAQAAAD4FiEoB+1wFIIAAAAA/yIE5WROECkIAAAA8CtCUA7a4ZgTBAAAAPgXISgHCyOwRDYAAADgX4SgLGKJbAAAAMD/CEE5CEFJUhAAAADgW4SgXCyRTTscAAAA4FuEoCxiiWwAAADA/whBWRRaVwqiHQ4AAADwL0JQFq0rBNEOBwAAAPgYISiLWB0OAAAA8D9CUA4WRuA4QQAAAIB/EYJycLBU5gQBAAAA/kUIyiLa4QAAAAD/IwRlEe1wAAAAgP8RgnJQCUoSggAAAADfIgRlEe1wAAAAgP8RgrIouO7V5DhBAAAAgH8RgrIolK4EUQoCAAAAfIsQlEUB2uEAAAAA3yME5WB1ONrhAAAAAP8iBOVgYQSWyAYAAAD8ixCURcF1paBkKt8jAQAAALAphKAsoh0OAAAA8D9CUE6OE0QIAgAAAPyKEJRFoXWlIEIQAAAA4F+EoFwskc2cIAAAAMC3CEFZxJwgAAAAwP8IQVkUYk4QAAAA4HuEoFy0w5GBAAAAAN8iBGUR7XAAAACA/xGCcrJEdr5HAgAAAGBTCEG5WCKbFAQAAAD4FiEoi9YVgmiHAwAAAHyMEJRFtMMBAAAA/kcIyiLa4QAAAAD/IwRlEe1wAAAAgP8RgrKIdjgAAADA/whBWRTKhCBSEAAAAOBXhKAsCq57NZkTBAAAAPgXISiLArTDAQAAAL5HCMqidYvD0Q4HAAAA+BghKIuYEwQAAAD4HyEoJ+1whCAAAADArwhBWcQS2QAAAID/EYKyKMTqcAAAAIDvEYJyUgkiBAEAAAB+RQjKIpbIBgAAAPyPEJSLdjgqQQAAAIBvEYJy0Q5HKQgAAADwLUJQFtEOBwAAAPgfISiLgmszEO1wAAAAgI8RgrIoxOpwAAAAgO/lNQTddttt2mmnnVRWVuZOEyZM0BNPPKHOKriuFJRK5XskAAAAAHwZggYNGqQbbrhBb775pqZPn66DDz5Yxx13nN577z11RusKQVSCAAAAAB8L5/PBjznmmA7fX3fdda46NGXKFI0dO1adtR0uSQgCAAAAfCuvIai9ZDKp+++/X42Nja4tbmNaW1vdKa2urk5+XCKbDAQAAAD4V94XRnjnnXdUUlKiWCyms88+Ww899JDGjBmz0dtef/31Ki8vz5wGDx4sP6EdDgAAAPC/vIeg0aNHa+bMmXrjjTf0ve99T5MmTdLs2bM3etvLLrtMtbW1mdOiRYvkJ6H0wghkIAAAAMC38t4OF41GNXLkSPf1+PHjNW3aNP3mN7/RH//4xw1ua9UiO/lVuh0uRQoCAAAAfCvvlaD1pVKpDvN+OhMOlgoAAAD4X14rQdbeduSRR2rIkCGqr6/XXXfdpRdeeEFPPfWUOqMAB0sFAAAAfC+vIaiqqkqnnXaali1b5hY6sAOnWgA69NBD1Rml5wQlOVgqAAAA4Ft5DUF//etf1ZWk2+E8KkEAAACAb/luTlBnRjscAAAA4H+EoCwKrQtBSTIQAAAA4FuEoCwKrns1aYcDAAAA/IsQlIvjBBGCAAAAAN8iBOXkYKn5HgkAAACATSEE5SAEJakEAQAAAL5FCMoilsgGAAAA/I8QlJMlsvM9EgAAAACbQgjKotC6UlCSFAQAAAD4FiEoi2iHAwAAAPyPEJSTJbLzPRIAAAAAm0IIyqIg7XAAAACA7xGCctAOx8FSAQAAAP8iBOWgHY4MBAAAAPgXISgnc4JIQQAAAIBfEYJy0A6XJAQBAAAAvkUIysHCCGQgAAAAwL8IQVnEwggAAACA/xGCcjAniCWyAQAAAP8iBGURq8MBAAAA/kcIyiJWhwMAAAC6aAhatGiRFi9enPl+6tSpuvDCC/WnP/1J3Vlw3atJCAIAAAC6WAj6xje+oeeff959vXz5ch166KEuCF1++eW69tpr1V19Oico3yMBAAAAkNUQ9O6772qPPfZwX993333aYYcd9Nprr+lf//qX7rjjDnVXn84JohIEAAAAdKkQFI/HFYvF3NeTJ0/Wscce677ebrvttGzZMnVXIdrhAAAAgK4ZgsaOHas//OEPevnll/XMM8/oiCOOcJcvXbpUlZWV6q4CLJENAAAAdM0Q9Itf/EJ//OMfdeCBB+rUU0/VuHHj3OWPPvpopk2uO2KJbAAAAMD/wl/khyz8rFq1SnV1derRo0fm8rPOOktFRUXqroJrMxDtcAAAAEBXqwQ1NzertbU1E4AWLFigm2++WXPmzFGfPn2k7r46HCEIAAAA6Foh6LjjjtM//vEP93VNTY323HNP/epXv9Lxxx+v2267Td1VcF0piClBAAAAQBcLQTNmzNB+++3nvn7ggQfUt29fVw2yYPTb3/5W3b0djiWyAQAAgC4WgpqamlRaWuq+fvrpp3XCCScoGAxqr732cmGouwqta4ejEgQAAAB0sRA0cuRIPfzww1q0aJGeeuopHXbYYe7yqqoqlZWVqbtiiWwAAACgi4agK6+8UhdffLGGDRvmlsSeMGFCpiq0yy67qLu3wxla4gAAAIAutET2SSedpH333VfLli3LHCPITJw4UV/96lezOb5OJdQuBVkxKNQuFAEAAADoxCHI9OvXz50WL17svh80aFC3PlBq+3a4dEtc+1AEAAAAoBO3w6VSKV177bUqLy/X0KFD3amiokI//elP3XXdVfvMwwFTAQAAgC5UCbr88sv117/+VTfccIP22Wcfd9krr7yiq6++Wi0tLbruuuvUnQ+WashAAAAAQBcKQX//+9/1l7/8Rccee2zmsp122kkDBw7UOeec021DUPv2tyQpCAAAAOg67XDV1dXabrvtNrjcLrPruqt2hSDa4QAAAICuFIJsRbhbb711g8vtMqsIdVcd2uG679QoAAAAoOu1w/3yl7/U0UcfrcmTJ2eOEfT666+7g6c+/vjj6q5C7UIQlSAAAACgC1WCDjjgAH344YfumEA1NTXudMIJJ+i9997TnXfeqe6qfTscc4IAAAAAfwp4XvY+rb/99tvaddddlUwmtTXU1dW5Zbpra2tVVlYmPxh+2X/dynBTL5+oPqUF+R4OAAAA0C3UbUE2+EKVIHx+SxyFIAAAAMCfCEE5WhwhmSIFAQAAAH5ECMrRvCAWRgAAAAC6wOpwtvjBZ7EFErq79AFTyUAAAABAFwhBNtHo864/7bTT1J3RDgcAAAB0oRB0++23524kXQTtcAAAAIC/MScoR5UgCkEAAACAPxGCcjYniBQEAAAA+BEhKMvWZSAlCUEAAACALxGCsiyQbodL5XskAAAAADaGEJRlocycICpBAAAAgB8RgnLUDkcIAgAAAPyJEJSrdjgyEAAAAOBLhKAcrQ5HJQgAAADwJ0JQjtrhWCIbAAAA8CdCUI4OlppkdTgAAADAlwhBWbYuA9EOBwAAAPhUXkPQ9ddfr913312lpaXq06ePjj/+eM2ZM0edGXOCAAAAAH/Lawh68cUXde6552rKlCl65plnFI/Hddhhh6mxsVGdvR2Og6UCAAAA/hTO54M/+eSTHb6/4447XEXozTff1P7777/B7VtbW90pra6uTv5dIptKEAAAAOBHvpoTVFtb68579uy5yfa58vLyzGnw4MHym9C6V5QQBAAAAPiTb0JQKpXShRdeqH322Uc77LDDRm9z2WWXuaCUPi1atEi+bYcjBAEAAAC+lNd2uPZsbtC7776rV155ZZO3icVi7uRnmXY45gQBAAAAvuSLEHTeeefpscce00svvaRBgwapMwuxRDYAAADga3kNQZ7n6fzzz9dDDz2kF154QcOHD1dn92k7XL5HAgAAAMB3Icha4O666y498sgj7lhBy5cvd5fbogeFhYXqjJgTBAAAAPhbXhdGuO2229wCBwceeKD69++fOd17773qrNZlIEIQAAAA4FN5b4frakJB2uEAAAAAP/PNEtldRaYdjhQEAAAA+BIhKMtohwMAAAD8zRdLZHcJNQulxlXq4dW4bykEAQAAAP5EJShbnvqx9OeDtFvT2oO90g4HAAAA+BMhKFvCBe4spjZ3TjscAAAA4E+EoCyHoKiXDkF5Hg8AAACAjSIEZTsEKe7OqQQBAAAA/kQIypZIuhLU6s4JQQAAAIA/EYKyXAmKpCtB9MMBAAAAvkQIyvbCCJlKUJ7HAwAAAGCjCEHZrgRlFkYgBQEAAAB+RAjK+pwgQhAAAADgZ4SgrFeCaIcDAAAA/IwQlKN2uCQpCAAAAPAlQlC2Q1BqbQjyaIcDAAAAfIkQlOU5QZ8ujJDn8QAAAADYKEJQzuYEkYIAAAAAPyIEZUu4cO1ZuhJEKQgAAADwJUJQtoRj7iySYnU4AAAAwM8IQdkSWVcJWrcwAu1wAAAAgD8RgrJdCVo3JyhJCAIAAAB8iRCU5TlBIS+hoFIiAwEAAAD+RAjKciXIxNTGwggAAACATxGCsrxEtokpzsIIAAAAgE8RgrIlFJaCYfdlgVWC6IcDAAAAfIkQlIN5QbGAVYIIQQAAAIAfEYJyMC+IShAAAADgX4SgHBwryOYEJVP5HgwAAACAjSEE5agS5FEJAgAAAHyJEJRNzAkCAAAAfI8QlKNKEO1wAAAAgD8RgnI0J4h2OAAAAMCfCEG5qAQFWB0OAAAA8CtCUDaFCzKVoBQZCAAAAPAlQlAOQpCbE0QlCAAAAPAlQlA2RT6tBDEnCAAAAPAnQlAu2uFsThCrwwEAAAC+RAjK0Zwg2uEAAAAAfyIE5WhOEO1wAAAAgD8RgnI0J4jV4QAAAAB/IgTlohIUaFOSFAQAAAD4EiEoZ8cJIgQBAAAAfkQIytmcoHwPBgAAAMDGEIKyKVLozqgEAQAAAP5FCMqmcMydMScIAAAA8C9CUDaFP60EUQgCAAAA/IkQlItKkNpohwMAAAB8ihCUozlBcdrhAAAAAF8iBOVoTlBjayLfowEAAACwEYSgHM0Jqm+J53s0AAAAADaCEJSDSlBMbapvoRIEAAAA+BEhKBdzggIJNbfFlUim8j0iAAAAAOshBOWgEpRuiWtgXhAAAADgO4SgHMwJ+nReECEIAAAA8BtCUDaFwlIglDlWUG0ziyMAAAAAfkMIytm8ICpBAAAAgB8RgnJ1rCC3QhyVIAAAAMBvCEE5PVYQlSAAAADAbwhB2UYlCAAAAPA1QlC2MScIAAAA8LW8hqCXXnpJxxxzjAYMGKBAIKCHH35YXaoSxHGCAAAAAN/JawhqbGzUuHHj9Lvf/U5dc04Q7XAAAACA34Tz+eBHHnmkO3Up7SpBdbTDAQAAAL6T1xC0pVpbW90pra6uTn6eE7SSEAQAAAD4TqdaGOH6669XeXl55jR48GD5uhLUTDscAAAA4DedKgRddtllqq2tzZwWLVok32FOEAAAAOBrnaodLhaLuZOvpStBATtOEO1wAAAAgN90qkpQpxBpXwkiBAEAAAB+k9dKUENDg+bOnZv5fv78+Zo5c6Z69uypIUOGqFNaVwmKqU3N8aTiyZQiIbImAAAA4Bd5DUHTp0/XQQcdlPn+oosucueTJk3SHXfcoU4pMyeozZ03tCTUozia50EBAAAA8EUIOvDAA+V5nrqUdZWgkuDaVjhriSMEAQAAAP5Bn1aO5gQVh9aGoDpWiAMAAAB8hRCUo0pQcWht+GFxBAAAAMBfCEE5mhNUGEi3w1EJAgAAAPyEEJSjSlBRYO3CCHVUggAAAABfIQRlW3Evd1bh1bhzKkEAAACAvxCCsq1soDvrkVipgFLMCQIAAAB8hhCUbWUDJAUU9uLqpToqQQAAAIDPEIKyLRSRSvq6L/sHVlMJAgAAAHyGEJQL5Wtb4ghBAAAAgP8QgnI4L2hAYDUHSwUAAAB8hhCUC+WD3Fn/QDWVIAAAAMBnCEE5DEFWCWJhBAAAAMBfCEE5bIezOUEcLBUAAADwF0JQTtvhqAQBAAAAfkMIymElqK/WKB6PK55M5XtEAAAAANYhBOVCSR95wbBCAU99VMPiCAAAAICPEIJyIRhSoHRApiWuurEt3yMCAAAAsA4hKMcHTLUV4j5e2ZDv0QAAAABYhxC0FVaIm7eyMd+jAQAAALAOIWgrVILmVlEJAgAAAPyCEJQrZellsqs1j3Y4AAAAwDcIQTmuBLl2uKoGeZ6X7xEBAAAAIATlfk6QtcPVtyZUVd+a7xEBAAAAIATlUPnadrjegVpFFXfVIAAAAAD5RwjKlaJKKVzovhwcqNJc5gUBAAAAvkAIypVAQBq4q/tyj+AHVIIAAAAAnyAE5dKw/dzZhOBsKkEAAACATxCCcmnYvu5sr+D7mreCEAQAAAD4ASEolwbtLi8UU59AjYobPlZ9SzzfIwIAAAC6PUJQLkUKFBi8R6Ya9PHKxnyPCAAAAOj2CEFbc14QiyMAAAAAeUcIyrXha0PQnsHZenNBdb5HAwAAAHR7hKBcGzheyVBMvQN1+ui96UqmvHyPCAAAAOjWCEG5Fo4pMGQv9+WYlpma9gnVIAAAACCfCEFbQXDUoe78qNAbeuKdZfkeDgAAANCtEYK2hh1OlKeA9gx+oJnvzlKKljgAAAAgbwhBW0PZAHlD1x44de+mF/XWojX5HhEAAADQbRGCtpLguJPd+XGhV/X4O8vzPRwAAACg2yIEbS3bH6tUMKLtgos0e+Zrak0k8z0iAAAAoFsiBG0thRXSqMPdlwe0vKBHZi7N94gAAACAbokQlIeWuBNDL+rOF9+T57FAAgAAALC1EYK2ptFHKVUx3B049cDq+/XChyvzPSIAAACg2yEEbU2hiIITf+K+/N/wY7r7uRn5HhEAAADQ7RCCtraxJ6itz04qCbRoryW3a8ZClssGAAAAtiZC0NYWDCp6+DXuy2+FntGjjzzA3CAAAABgKyIE5cM2B6t5xBGKBpL64arL9c7Lj+Z7RAAAAEC3QQjKk8JT79BHZRNUFGjVds99V6mPX873kAAAAIBugRCUL5FC9fzOfXrW201RxdX073OlRFu+RwUAAAB0eYSgPKqsKNNHe9+kKq9CJY0LVP3cb/I9JAAAAKDLIwTl2XcPGaf7Kr7jvi547Vdqql6c7yEBAAAAXRohKM8ioaC+9p1L9I5GqUjNev+vZ6uhqSnfwwIAAAC6LEKQD/QtL5KO+qWSXkDjG1/WJzcdqFnvvZPvYQEAAABdEiHIJ3bc42DNPfhPqlOxdkjN0aD7jtBt9z6ihtZEvocGAAAAdCmEIB8ZfcDJCpz9khYVbKuegQYdN/tCfeNXD+mVqdOk+yZJj/9QSqXyPUwAAACgUwvnewDoqLTfSJVe8LQab5uoAXXz9JuWy9XnvzVSoHXtDQp7SAddlu9hAgAAAJ0WlSA/Kuyh4jMelFfUW8ODK1QcaNWc1KC11714g2Y9d4+SKe/T2y+eLi2dmbfhAgAAAJ1JwPO8dp+mO5e6ujqVl5ertrZWZWVl6nKWviU9e62W9ztIF8wdr6OW/D9NCj+jOq9Q94ePUfl2++vQuodUvuhZKRCSvvOkNHiPL/ZY9msQCGT7GQAAAAC+ywaEoE7kgyWrVHj3CRra8PZGr68v6K+V33hGQ+reVPiZK6RRh0pH3SgFQ599x2/eIT37U+nQa6RdvpWbwQMAAAA5RAjqyuItanvnIVVPvVelVdP0Zmq0ftV6nH4buVVDg1VakOrjztNmlB2sp0ddpeGNb2tI4ztqLByg2tJRKhoyTjsM7qVBa6Yq8M8TJC8phWLSWc9Lfcd2fMyWWqmgvONltkBDkG5KAAAA+AMhqBuxzbd4TbNmTXtBh7/+bYW1dkntR5J768jgG4oGkmryYipKL6ywzkqvXPcnD9A3ws+pQg1KREoUjje4gDR537tVkGzU4JUvaviiB1W6aqaaRx6twHG/U0EwKT16vrTgVaWO+pUWDDhKvUqiKi2IfN5ApddukWb+S9r3ImncKRs8j9ZESrFwUIF8teXVLZPee1Da/hipYsiG19MyiC/Kdho0rZZKeud7JAAAdFmEoO7q7XvlTf2zave4QB/32Ffx2Y9rtzcuUMiLqylUpo/K9lRx22r1b/5Ixan6zI/NTI3QOW0X6pHYFeodqFWdV6SyQNMGd/9xqp9Kgq3qozWZy36TOEE3J07QkMoSjelfprEDyjSwokCJhdPUc8lzaor20ppeu2v3pXdq+5VPZH7u5Z4nauZ2l2jHoZVaWdei119+WiWrZyk+YA9949ivaMzAci1YWavlSxaoes0aNTTWq0fvARo5Yhs1tXmaPX+R1tQ1aOTw4dpteKUqiqIbjLdmyYdqeut+9UitUaHXIhVVyhtznBord1TJ+qFtxWx5/zxRgfql8sKF0v6XKDDhXClSsDYcvXiDvLfvkWKlClQMlUYeIu37f2uvT0u0Sh8+ubZy1mtbqfd2UmHFZ2+zpmp3nwp9Tohcz5rGNkXCQZXEsrjAY/1yqa1R6jniy4e9+hVSMCwVVypv7LkEw1rakNK7S2p18HZ9FA7loXoZb5b+eaK0cIp03O+knU/d+mMAgCxqTSQVDAQUycffVKArhaDf/e53uvHGG7V8+XKNGzdOt9xyi/bY4/Mn+BOCNsPyd6X6ZdLwA6TwuqCQjEsfPKbU1L+osb5Wv+93rSYvDmkfvaWr665yN0kpoIXhYXo6dIDeiQ/UZck/aEBgtbtubmqApqS217fCz7rva70ifeAN0WqvTDHFNSKw1K1qt76EF9RTqd10dGiq+94qVAu8vioLNGrguvs2H6UGqjlQoG21UAWBeIf7SHprP5yHAmt/beNeSCvUQ8u8Sq1QLzUGixUIBDXYW6IJemejL4m1DL4VHqfqXuMVKSpTeXyVJi79k4q9BjV6Mbcan3ssBbUm0k9lidWKeh0raWZxeKgmDzpHleFWDWt5X6NWPKGCeM2nY1VQU0O7anLhEVpdPFJFRUXqF4trsFZoQPMcDV7xrAa0zNWaUC9NG3iaFvQ/Qi1rlihcv0RlsYB6FoY0qHWu+q95U0UtyxQPlagxVKpX4tvpjjU7qleoSWdUzNT2keVKlQ5Qqnyo1sT6aVmgr+oKBirUY6DKCgtUVhhRcTggb85/1XP2PxRqq9fSit20pufOGliU0MDAKoU/flax5W+6cdfF+mpOyV5aOuQYFY3cT4N6FqlvWYEqCiMuG7UlU1qwukkfV9Wpf+t8jWmbpUi8ce2H+7KB0oy/rz2mlc1FO+BSeXudo5XNKS1fXafypvnq1ThPoVBIDX3HqyHWX01tSTXHExpYUaR+pRGptV5KJWVbOB6tUDzluf9oo80rpbYGqcfwTCtm+s9XYMW70so50tB9pOJe0ht/kPfCL9QSiOry5m/rwbbdtf+2fXTL13dReVFEWj1vbWjts/3nB75FU6VXfyMlWtzz8ywIt9a51lQN2VMafZRUNmDTFaD7J0nvP7p2vMGI5h91lyq2P1A9i9cL7qmkVP2xVNJHXqzMFR6DwU2PraktoY9XNmpU3xLFwp8z729Tofftu6VgZO1cwM8L7LmSTEheSvFA2IX73qWx/FWDuzt779kiO9Ei+UlLPKloKPiZ7wdsPVPnV+ucf73p/u787fTdNbpfab6H1HU+r5X0zW3HgO14nfrntf93jThQXVGnCkH33nuvTjvtNP3hD3/QnnvuqZtvvln333+/5syZoz59+nzmzxKCcrQinX2467ejFCvJXOw1rlL8ictVryJNGX6OquNRHdT0tAa+fpUC9gF4Pa2BAn1SuZ8LBQPq3lZbsFCPjf65avtN0NCVz+uA969VQaI2c/u2UJHifcYpuny6It6nwSehkFqDRUoGIypK1CqsZOY6C2pB91F541JeQG8Ed9KM+FA1eQXaPrhQE4MzVBho2+jtp6W21fneJdor+ZZ+FLlb/QKfVrymp7bVrxJfU51XrDHBT/TD8L2uarbBy+f11NzUQG0TXNoh2OWDBcQqVSjpBV07ZK9A3ef+TKsXViywtqXSzEv112Kvt/oE1qhYLWpUgVoVUQ81qE+gpsNrGVdYs7wRGh/4sMN9rvB6KCBPlarNhNe05V4PLfd6qtorVd/AGm0TXKYCfXqfVpX82Ouv3oGazOtZqxLNDo12YXleWw8dHp6hvQOfBt76YJlKUx2f60vJHTXDG6VoUYWOjUzVoMZ3144t1E9vhHdTa2FfRYvKVZisU3HLMgWTbWoMlatnfIV2bXr5c1+3ZaH+qgoP0OrYYNUUDFJD4QBFEg0aWTdVu9dPVlwRfRjeVmMT76naK9F9yQO1fUG1ekQSag3EFEm1alTreyr2Gl14nuVtozeTI1UdrFQqWqaxJXXaNrpaRYkaBVvr1NIW19LWArcDoiCU0sCSgLzCSi0JD9HKUF+FIxG3HUvrPlLvhjkKKKWGaG+1hMvlJdpUklitXZpez7TPtoRKNK3yOK0K9lKzF1FzKqymVFjlatDI4DL1Uo3qS4arunyMapva1Fq9SNHWNSoJJVQcSigSDisUiWlNqFIfeYOUSCS0Z2KaRjS/q5pwLy0KDVFDQV8VlVSoqLBIijcp1LJa/aqna2DNdEVTzVrjlWqZvX+Cw1TbY6xCZQMULalQsKBMzcFiNXlRtTY3K9FcqxGJeRrVNttt55ZYpVqjPeUFw7KPyK2hYrft7FQfqlBjKqKCVbPUp+Ztt4OjwMZQWqGAVWAjRe5wA8lEm5KJuFK2gyjRqmiiXtFkk+LhEjXHKhUOSOWtyxVrq1ZdsEyrvArFI6UKFZQqWFCqVKRYXqRYqWiJUpEitTbWK163XIFEq8p6VKqioqeSLfVqq1+toJKKRaIKhMOuqt1oGVBBBcMR9zxXBHq5FUDLEtUqT6xWSTSosuJCFcaiCoTsL2BI1c0pVTcnlYyUqKCkhwoiQRW2rVYsXqfCiFQUDihev1KtqxYq1LBEpa3LVRCvVW3v8aobfrS8soEqaF7hqqVrUoVqamrU8Hl3atTy/yoZCOu9ASdp7pBTVBZqVUVitSLhkMKxQoWiRYoUFCkYLVQyGFMiGFM8GFMyGFWorUFFDZ8o1LBMC5qi+rChUIGCcvXt1VODC1rUu+49xWo/1kctFXqtoY/aYj21/YAKDS4Lq7m2Si31q1VYVKKy8p6qTK1W2Zp31bh6iZ5d1UOPLe+povKeOmyHgdqjrxSrX6hI0woFbC6rAoqXDlZLnx2VLKhUpGm5gjULpZXvK7T6I/f3YEHBaK0pGqFBfSo1sE+lmlIRVcdDioZC6lkYUHksqFgwpUggqeaWVjU2tyhR9aHCy99Sa91KvaKd9Z+G7TWsbw9XUR4zoEwBL6VQvEHhuoVux5X7XSrpp0AqpUjtPIWaVineYxvF++wgFfZ077OA5ymYbFJq5Vy1vXWvei54UqlAUHUV26u5cgc19xqreOUY91rHglIs7CkW9BQMhdxOMHuto4GUCtXk5vG2eWGlFFFxUaFKCguUiLeoualBzS1takoFtaY1oE/WtGnhmhYVRcMaWFGoQT0KNbBHoXqXxNTQmtBq6yoISBWqU2GqQclUQHEFlEja/78BBQMhFcQiCimp1rrVmvHhAv3mpaVamSzSGpUqXFCsv52+h3YbtvY5xpMpNbau/btibfKhdcHVPmq2JRJqa21VWyLl/q9pjSeUXLNQ3ppPFIiVKdh3exUWl6jIa1JhS5XbQZUMRN3vZDIQWXuesp2hKaWa66SWGgUihQpVDFasoEixSNB9RmhYOEut819XcM18RVpWKZxsUWu/XRQfvL9ifbdRaWm5QtECtcRT7jVorlutRNUHKq6bp5K6jxVNNrgdXqmS/lrT0KzaujqFQkFVlJaqsKRMNSp1/2d5RZUKlfRWQVGxCiMhFURC7tza+j3bsVOzVMnWRsWDBWoLFqg+FVFjPKjixBr1aF2iWCSkeOX2SrXUqei5n6jgw0flhQrUMu40tY0/U6oYrHB47WtoVbe151I8kVT1mmrV1dWqqKhYPXtUKLbyPXkfPC7VLFTbkP3Uus3hihbb/2mN7mdUUCZ9/MLa6QwNa3dSrxx1shbu/AMNrixR78KAArYDMRyTF29Wc9U8JRprFIiVrP07V9ZPYXvfe57i8YT721EcDbi/ubbHzrZ7c1tCZZX9lG+dKgRZ8Nl999116623uu9TqZQGDx6s888/Xz/60Y8+82cJQT5ge9NtD3zV7LV7EcMFa/cojzjo0xBlocrCSqTw05+zDxtrFkjV86RAUBq279rrW+pU/fZ/XcWn58jdFer56V5/21Oeqq9yO+4DdtBYa7dqWK7GlQsUr14sr3aJvNZ69yb1IkUq3fVEFfbZRqsaWvXGx9Vqjic1tldQg2umq27OSwotmep+31rDpaovH63wwZdp9KA+SqQ8raht1oKF81Q1/301JgIauOP+ru3O3vA1TXFVLV+qyteuUWXVFFVH+mtJZIjeK9lbc0r3UmVpodsztm14mXp+cK96fvKYwq01Cqbi7j8xu31VZJBW9N1PiREHq8eiydp+7l9UEV+hpkgPNRX0V6vCakt6Wh7sr/eiO2phaIhKgm3qr1U6wJuqwatfVypcoPcrDtSUxChFW1aqR9syDVaV+qtKlfEVmQ+4aXWBUr1cdoxqS0ZoZMN09W2ep6pEkRbGyzRLo/ROyb6q6NFL+0bnaLfGlzR61TOKWRvhZ2hSgaYlt1UsENdewffXblovoJsSp2ilynVZ+C5VBj5tvaxXod5PDVFUce0Q+EThQGqzf9Us1LYpvEF10P0aekHN9QZq28BiBQOeVnll+mXiFA0KVuvc8CMKeYkNAqKFjY3d18Ye977kAXrT21YDtFrFgRYXziyAHxB6W7sE5rrH/CzfbztXT6d2033Ra7VTcP4mb9fqWXj5/DFli4X7MjVq2+CSrfaYQGfU4BW4nUC2k6ZA8S16n9rfG9uhF1Sqw06mLWV/W9ffkbS5P5fayGEh7e9pmyIqUbMLgF+EhZk6t4ssqlZFXTiMKOHuz86j9rUSLpys/xjrPx/7W9ukmEoCn/3/zvrs59aoRBElVWi76TbjubR5Iff/l/0/0P7/qC/CHt+ehW3jBhWqWTH1Vu1Gf0fstu3/v7DXwF6dTf0+2Y4ue23bvIgigYTbVlv6+qzPdjb11ZqN/r9V7xW6rbmx62wHnn0GKlfjRq93/39ds0r51mlCUFtbm2sReuCBB3T88cdnLp80aZJqamr0yCOPdLh9a2urO7V/ohaYCEHo9KxtytqtNrcNxUKkhcdNLX9urVXWBmlzc7zU2pavPmM2ev+bbDWxUDvnSSnZpnhRHzUHCt0e/GCiRcUVvRUo7SuvbJA+qYlrUXWTBtbNVJ8Fj6l1u+PVMmBP187lNdeoYOUs9ajsq0hFf1fqt3CXSKXcPK3Ayg/W7pVqXKXWWKU+TA3UymAv9akoUa/CgIobFihaO0/xWA819NxBDYmwvOWzFFn+lsoaF6i4caFaykZo3ohvaVmgj9RUpeI1c1TfcwcFCiu048ByDU0tlmY/qubVC7R48UItLhqrTwYfp+LSCo2se0O9qmco0bhGyeZatUXK1Fo8QF64QLG2GgW9pKq2OVGh/juoojCqiqKI29vY3JZ0bXx2itdVKbR6jsI1nyhaN19F9Qtc+2JruEwtBX20tO9BWjHwELdXdOceLer/5q9c9cfaMmtTBQqnWl37V02PHVVfsb0GBGs0rG6aimrnKlW3TImmGtfu+VFbpeojPRUp6qHK0gLtWJlSr3CrFtQm9M6yJhW0rtSgxEKVxVfJS6Vkx1OuLxmm5l47KBUudHvoI221CoRjbq/1R6W7a7a2cXv1drbQW/uaYvYBwrMPLW2KpNrc3sul4cGqSpWpd8t8DWyZ69pqE8X95RX1cv/ZN6VC7j5S8RZVJlaoX+snCnutmle6h2bFdlVlsF5DEgsUa10tr6VBnv0+WeU3VKhlxdtrQcVeKuo1RBP6JjQ0UKXqedMVXzpLwabVbk+77ZW1hVqiXourPCRDBVpVMFQfRse4FtiSxBqVpmxbpdweySI1qyxVq9JkrUqStSpINWpNyTaq6z1eNcEeqq2pVltTrWLJJkVTTa591toU3U6VUESpoFVkStQSLFJhqtG1w1q1yH6/qlWuftEm9Q/WuXEFrRKQaFYs1aRYqlkFdu41uz2/TZGe7jzUVqdoolEtwUK1hMuUCITlJZMKeAm3lz8SsDpQUkEv4eZqWuXF7XUPFqouXOk+rCmVcFWPkJ3ch8mU+1Bpj2ffu/exYq5KlbCA70kNwTLVx/qoIdZP1eHerv14TOMUjWuZ7j6U1tnHqUChStSomNeqD0r21BsDJqkgVa99l9yhoc3vqi5YodVWjZRctTLqtbnW4KjaXOuz3U97q9TDVbJ6hprdXEx7fW1ngVWJPwoM1TwN0ohojYanFrrXxP42JRRUQ6hcLaFSBVNtKkg2qMYr0TupYVoVrNS+Zas0XIvX7lFva3NVsiWBfloe6OWqYrbdh2mxRnvz3QfgVapQVWBdVbRgmPoG67RN/EO3kyiYbHHjt7FvjL3W7hUOhLQq0FMfhUfLixZrQtsUlcZXbvRnrA18mXq7Grn9hngK6BP1V7XKtI0Wa0igasO/uV5E06O768O+RykRLlZF3Qca0DRHQ+NzNTC5JNPhYN0OVsm38PRFws+Wsg/AJuQikz26t+5rz4WFGhWrUcXqFY2rOFWnQHLjXRVbwoKYtXCXeg3qqU+7K2xHk1kbpOIbPH8Llha+bJuvv/CT/Y68E9hWn4SGq8qrUMrztIv3nnb13ldFYMPOFbNclfpYAzUn0V+1KnZb0DogvFDEVUBTKU+ptiYVes3qFWpQz0C9yr36Dd4D6++cs1BXaH9P2wUze2UtiNh7uG9gbQv9W6mRuiL5XfX06nRu+CHtHpjzuTvX2nfD2LZ7MTVOH3v9dHBwpnYIfrLB7S1w/S15pG5KnKxxwY/1y9hfNcxbkhlr+52SFvqt2mXdHmX2LDYj9FsIil69Mu/tzJ0mBC1dulQDBw7Ua6+9pgkTJmQu/+EPf6gXX3xRb7zxRofbX3311brmmms2uB9CEACgS7AdGPEmKVry+fPV7L9vW3jDAmC79uXPtLHK/MZWvrS5WqHPWXjFbmM7b+xk4dpaDNcfn3UL2M6aLVz8Ze2Pe5v/gcpeNzul575+3m3tdTM2rmBYiZQUT8m1Fm7wmLaTauX7LozKFs6xBXHsPFr8+TuubGdSa4PbmeR2XFkXg/3cpp6XvabGXrP0bex1tIVeXLdFTJ79blj7p40n2aZEvE2Nzc2uXclaCoO23ezx3Cm+9rxd67iF+rqmVhWHPRcwbLtZW2lLKugWj4mEAooE1+4YS6/cau1O1lKXbm/LjKl5jdZUVynZ1uxe01g0osKCQnnBqOoTATUlggpHY4pECxSN2XlMUWvrSrWtfT1tDkx6Z17DSnnNa9RW3E8tKnQvV2hdG5iFMavoZ/bVWdeJvT7W+dG4Uon6KlfViiuq4t6DXYvuxiTaWlVfX6d4c72K1azCYFLBnsPWtou5Te2pJZF0T88URUOZ3we7zubEWttb5jVIzw+1oBhvVWtTneJNdVJxbwV7DFYkElXYxu8lFEisfY3sdyAVjLr7CjWucK2TwX5jM6+DvebJRELJpmp5jdVKJVqUircqFQgrHi5WqKBMZRU9FYwUuJ1KDfU1SoRKFIpG3XZzr1dLtVqTnhpV5Lpb3LQF2+FjvzuSm5Mas0Ut4k1qC8S0oq7V7bQMNq9StLiHSir7Kxax9kNPyWRKyeY1StUsdm274ZJe8qKlqm9NulNBNKwS2+6xsKKRLC7W9AV12RBEJQgAAADAlw1BeY1svXr1cqtErVjRcSUx+75fvw0nV8ViMXcCAAAAgC8qrwu8R6NRjR8/Xs8+u3apZWMT1e379pUhAAAAAMiWvDfvXXTRRW4hhN12280dG8iWyG5sbNQZZ5yR76EBAAAA6ILyHoJOOeUUrVy5UldeeaU7WOrOO++sJ598Un379s330AAAAAB0QXk/TtCXwXGCAAAAAGxpNsjrnCAAAAAA2NoIQQAAAAC6FUIQAAAAgG6FEAQAAACgWyEEAQAAAOhWCEEAAAAAuhVCEAAAAIBuhRAEAAAAoFshBAEAAADoVsLqxDzPyxwdFgAAAED3VbcuE6QzQpcNQfX19e588ODB+R4KAAAAAJ9khPLy8s+8TcDbnKjkU6lUSkuXLlVpaakCgUDeEqeFsEWLFqmsrCwvY0D2sV27JrZr18M27ZrYrl0T27VrqvPRdrVYYwFowIABCgaDXbcSZE9u0KBB8gPb6Pne8Mg+tmvXxHbtetimXRPbtWtiu3ZNZT7Zrp9XAUpjYQQAAAAA3QohCAAAAEC3Qgj6kmKxmK666ip3jq6D7do1sV27HrZp18R27ZrYrl1TrJNu1069MAIAAAAAbCkqQQAAAAC6FUIQAAAAgG6FEAQAAACgWyEEAQAAAOhWCEFf0u9+9zsNGzZMBQUF2nPPPTV16tR8Dwmb6eqrr1YgEOhw2m677TLXt7S06Nxzz1VlZaVKSkp04oknasWKFXkdMzb00ksv6ZhjjnFHh7Zt+PDDD3e43tZ+ufLKK9W/f38VFhbqkEMO0UcffdThNtXV1frmN7/pDvJWUVGh7373u2poaNjKzwRbsl1PP/30Dd6/RxxxRIfbsF395frrr9fuu++u0tJS9enTR8cff7zmzJnT4Tab83d34cKFOvroo1VUVOTu55JLLlEikdjKzwZbsl0PPPDADd6vZ599dofbsF395bbbbtNOO+2UOQDqhAkT9MQTT3Sp9yoh6Eu49957ddFFF7llAWfMmKFx48bp8MMPV1VVVb6Hhs00duxYLVu2LHN65ZVXMtf93//9n/7zn//o/vvv14svvqilS5fqhBNOyOt4saHGxkb33rMdEhvzy1/+Ur/97W/1hz/8QW+88YaKi4vd+9T+gKfZB+X33ntPzzzzjB577DH3Afyss87ais8CW7pdjYWe9u/fu+++u8P1bFd/sb+j9qFpypQpbpvE43Eddthhbltv7t/dZDLpPlS1tbXptdde09///nfdcccdbkcH/LtdzZlnntnh/Wp/m9PYrv4zaNAg3XDDDXrzzTc1ffp0HXzwwTruuOPc39Qu8161JbLxxeyxxx7eueeem/k+mUx6AwYM8K6//vq8jgub56qrrvLGjRu30etqamq8SCTi3X///ZnL3n//fVtO3nv99de34iixJWz7PPTQQ5nvU6mU169fP+/GG2/ssG1jsZh39913u+9nz57tfm7atGmZ2zzxxBNeIBDwlixZspWfATZnu5pJkyZ5xx133CZ/hu3qf1VVVW4bvfjii5v9d/fxxx/3gsGgt3z58sxtbrvtNq+srMxrbW3Nw7PA521Xc8ABB3gXXHDBJn+G7do59OjRw/vLX/7SZd6rVIK+IEu2lo6ttSYtGAy6719//fW8jg2bz9qirN1mxIgRbq+xlW6NbVvbm9V++1qr3JAhQ9i+ncj8+fO1fPnyDtuxvLzcta6mt6OdW6vUbrvtlrmN3d7ez1Y5gn+98MILrsVi9OjR+t73vqfVq1dnrmO7+l9tba0779mz52b/3bXzHXfcUX379s3cxiq7dXV1mT3U8Nd2TfvXv/6lXr16aYcddtBll12mpqamzHVsV39LJpO65557XHXP2uK6yns1nO8BdFarVq1yvxTtN66x7z/44IO8jQubzz4IW2nWPkBZaf6aa67Rfvvtp3fffdd9cI5Go+5D1Prb165D55DeVht7n6avs3P7IN1eOBx2/4Gzrf3LWuGs9WL48OGaN2+efvzjH+vII490//GGQiG2q8+lUildeOGF2meffdyHYrM5f3ftfGPv5/R18N92Nd/4xjc0dOhQt9Nx1qxZuvTSS928oQcffNBdz3b1p3feeceFHmsft3k/Dz30kMaMGaOZM2d2ifcqIQjdln1gSrPJfxaK7I/0fffd5ybQA/Cvr3/965mvbW+jvYe32WYbVx2aOHFiXseGz2dzSGyHU/t5mOi627X9XDx7v9pCNfY+tR0Y9r6FP40ePdoFHqvuPfDAA5o0aZKb/9NV0A73BVlJ1/Y2rr8Shn3fr1+/vI0LX5zt0dh22201d+5ctw2t5bGmpqbDbdi+nUt6W33W+9TO11/MxFavsZXF2Nadh7W02t9le/8atqt/nXfeeW6hiueff95Nvk7bnL+7dr6x93P6Ovhvu26M7XQ07d+vbFf/iUajGjlypMaPH+9WAbTFan7zm990mfcqIehL/GLYL8Wzzz7boQxs31vpEJ2PLZ1re6VsD5Vt20gk0mH7Wune5gyxfTsPa5WyP7btt6P1I9uckPR2tHP7Q249zmnPPfecez+n/6OG/y1evNjNCbL3r2G7+o+tcWEflK2lxraFvT/b25y/u3ZuLTrtA66tSGZL+FqbDvy3XTfGqgum/fuV7ep/qVRKra2tXee9mu+VGTqze+65x60ydccdd7iViM466yyvoqKiw0oY8K8f/OAH3gsvvODNnz/fe/XVV71DDjnE69Wrl1vZxpx99tnekCFDvOeee86bPn26N2HCBHeCv9TX13tvvfWWO9mftF//+tfu6wULFrjrb7jhBve+fOSRR7xZs2a5FcWGDx/uNTc3Z+7jiCOO8HbZZRfvjTfe8F555RVv1KhR3qmnnprHZ4XP2q523cUXX+xWIbL37+TJk71dd93VbbeWlpbMfbBd/eV73/ueV15e7v7uLlu2LHNqamrK3Obz/u4mEglvhx128A477DBv5syZ3pNPPun17t3bu+yyy/L0rPB523Xu3Lnetdde67anvV/tb/GIESO8/fffP3MfbFf/+dGPfuRW+LNtZv932ve2uubTTz/dZd6rhKAv6ZZbbnG/BNFo1C2ZPWXKlHwPCZvplFNO8fr37++23cCBA9339sc6zT4kn3POOW5JyKKiIu+rX/2q+8MOf3n++efdh+T1T7aEcnqZ7CuuuMLr27ev22kxceJEb86cOR3uY/Xq1e7DcUlJiVu+84wzznAftOHP7Wofruw/VvsP1ZZpHTp0qHfmmWdusAOK7eovG9uedrr99tu36O/uJ5984h155JFeYWGh23FlO7Ti8XgenhE2Z7suXLjQBZ6ePXu6v8EjR470LrnkEq+2trbD/bBd/eU73/mO+9tqn5Hsb63935kOQF3lvRqwf/JdjQIAAACArYU5QQAAAAC6FUIQAAAAgG6FEAQAAACgWyEEAQAAAOhWCEEAAAAAuhVCEAAAAIBuhRAEAAAAoFshBAEAAADoVghBAIBuIxAI6OGHH873MAAAeUYIAgBsFaeffroLIeufjjjiiHwPDQDQzYTzPQAAQPdhgef222/vcFksFsvbeAAA3ROVIADAVmOBp1+/fh1OPXr0cNdZVei2227TkUceqcLCQo0YMUIPPPBAh59/5513dPDBB7vrKysrddZZZ6mhoaHDbf72t79p7Nix7rH69++v8847r8P1q1at0le/+lUVFRVp1KhRevTRRzPXrVmzRt/85jfVu3dv9xh2/fqhDQDQ+RGCAAC+ccUVV+jEE0/U22+/7cLI17/+db3//vvuusbGRh1++OEuNE2bNk3333+/Jk+e3CHkWIg699xzXTiywGQBZ+TIkR0e45prrtHJJ5+sWbNm6aijjnKPU11dnXn82bNn64knnnCPa/fXq1evrfwqAAByLeB5npfzRwEAdHs2J+if//ynCgoKOlz+4x//2J2sEnT22We74JG21157adddd9Xvf/97/fnPf9all16qRYsWqbi42F3/+OOP65hjjtHSpUvVt29fDRw4UGeccYZ+9rOfbXQM9hg/+clP9NOf/jQTrEpKSlzosVa9Y4891oUeqyYBALou5gQBALaagw46qEPIMT179sx8PWHChA7X2fczZ850X1tlZty4cZkAZPbZZx+lUinNmTPHBRwLQxMnTvzMMey0006Zr+2+ysrKVFVV5b7/3ve+5ypRM2bM0GGHHabjjz9ee++995d81gAAvyEEAQC2Ggsd67enZYvN4dkckUikw/cWnixIGZuPtGDBAldheuaZZ1ygsva6m266KSdjBgDkB3OCAAC+MWXKlA2+33777d3Xdm5zhayFLe3VV19VMBjU6NGjVVpaqmHDhunZZ5/9UmOwRREmTZrkWvduvvlm/elPf/pS9wcA8B8qQQCAraa1tVXLly/vcFk4HM4sPmCLHey2227ad9999a9//UtTp07VX//6V3edLWBw1VVXuYBy9dVXa+XKlTr//PP17W9/280HMna5zSvq06ePq+rU19e7oGS32xxXXnmlxo8f71aXs7E+9thjmRAGAOg6CEEAgK3mySefdMtWt2dVnA8++CCzcts999yjc845x93u7rvv1pgxY9x1tqT1U089pQsuuEC77767+97m7/z617/O3JcFpJaWFv2///f/dPHFF7twddJJJ232+KLRqC677DJ98sknrr1uv/32c+MBAHQtrA4HAPAFm5vz0EMPucUIAADIJeYEAQAAAOhWCEEAAAAAuhXmBAEAfIHubADA1kIlCAAAAEC3QggCAAAA0K0QggAAAAB0K4QgAAAAAN0KIQgAAABAt0IIAgAAANCtEIIAAAAAdCuEIAAAAADqTv4/DGEufyc6dEsAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_losses(train_loss, val_loss):\n",
    "    epochs = np.arange(1, len(train_loss) + 1)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(epochs, train_loss, label='Train Loss')\n",
    "    plt.plot(epochs, val_loss, label='Validation Loss')\n",
    "    plt.title('Losses over Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_losses(history_xavier[\"train_loss\"], history_xavier[\"val_loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Xavier_Model_1e-3 | MSE: 0.9149 | MAE: 0.5625\n"
     ]
    }
   ],
   "source": [
    "def calculate_model_performance(model_name):\n",
    "    \"\"\"Calculate and print model performance metrics\"\"\"\n",
    "    model = LinearlyActuatedStrutsXavier() if model_name == \"Xavier\" else LinearlyActuatedStrutsHe()\n",
    "    model.load_state_dict(torch.load(f'best_{model_name}.pth'))\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(X_test)\n",
    "    \n",
    "    # Rescale predictions back to [0, 73.8]\n",
    "    pred_real = predictions * 73.8\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mse = nn.MSELoss()(pred_real, y_test * 73.8).item()\n",
    "    mae = torch.mean(torch.abs(pred_real - y_test * 73.8)).item()\n",
    "    \n",
    "    print(f\"Model: {model_name} | MSE: {mse:.4f} | MAE: {mae:.4f}\")\n",
    "    return pred_real, mse, mae\n",
    "\n",
    "\n",
    "xavier_pred, xavier_mse, xavier_mae = calculate_model_performance(\"Xavier_Model_1e-3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[73.6000, 73.8000, 73.8000,  0.8000,  0.1000, 69.2000],\n",
      "        [ 0.1000,  0.2000, 57.6000,  0.2000, 47.8000,  0.0000],\n",
      "        [49.5000, 29.0000,  0.4000, 17.1000, 24.1000, 22.9000],\n",
      "        ...,\n",
      "        [73.4000, 73.6000, 73.7000, 53.8000,  0.1000,  0.3000],\n",
      "        [73.7000, 73.7000, 73.7000,  0.1000, 73.7000, 54.7000],\n",
      "        [73.7000, 69.9000,  0.0000, 44.9000, 66.6000, 56.9000]])\n"
     ]
    }
   ],
   "source": [
    "xavier_pred = torch.round(xavier_pred, decimals=1)\n",
    "print(xavier_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[73.4300, 73.3600, 73.2900,  0.0000,  0.0000, 69.4700],\n",
      "        [ 0.0000,  0.0000, 57.5600,  0.0000, 48.9800,  0.0000],\n",
      "        [49.2700, 28.4200,  0.0000, 16.8800, 24.0200, 23.3700],\n",
      "        ...,\n",
      "        [73.8000, 73.7200, 73.8000, 54.3900,  0.0000,  0.0000],\n",
      "        [73.8000, 73.8000, 73.8000,  0.0000, 73.8000, 54.6100],\n",
      "        [73.5800, 71.1300,  0.0000, 44.6500, 67.7400, 56.7000]])\n"
     ]
    }
   ],
   "source": [
    "y_test_scaled = y_test * 73.8\n",
    "print(y_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will try with smaller learning rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/600 | Train Loss: 6.8989 | Val Loss: 3.3473\n",
      "Epoch 11/600 | Train Loss: 0.2090 | Val Loss: 0.2206\n",
      "Epoch 21/600 | Train Loss: 0.0815 | Val Loss: 0.0795\n",
      "Epoch 31/600 | Train Loss: 0.0516 | Val Loss: 0.0643\n",
      "Epoch 41/600 | Train Loss: 0.0408 | Val Loss: 0.0371\n",
      "Epoch 51/600 | Train Loss: 0.0361 | Val Loss: 0.0427\n",
      "Epoch 61/600 | Train Loss: 0.0357 | Val Loss: 0.0419\n",
      "Epoch 71/600 | Train Loss: 0.0313 | Val Loss: 0.0240\n",
      "Epoch 81/600 | Train Loss: 0.0351 | Val Loss: 0.0267\n",
      "Epoch 91/600 | Train Loss: 0.0255 | Val Loss: 0.0277\n",
      "Epoch 101/600 | Train Loss: 0.0209 | Val Loss: 0.0224\n",
      "Epoch 111/600 | Train Loss: 0.0234 | Val Loss: 0.0196\n",
      "Epoch 121/600 | Train Loss: 0.0216 | Val Loss: 0.0226\n",
      "Epoch 131/600 | Train Loss: 0.0182 | Val Loss: 0.0184\n",
      "Epoch 141/600 | Train Loss: 0.0170 | Val Loss: 0.0173\n",
      "Epoch 151/600 | Train Loss: 0.0191 | Val Loss: 0.0183\n",
      "Epoch 161/600 | Train Loss: 0.0168 | Val Loss: 0.0187\n",
      "Epoch 171/600 | Train Loss: 0.0159 | Val Loss: 0.0161\n",
      "Epoch 181/600 | Train Loss: 0.0180 | Val Loss: 0.0176\n",
      "Epoch 191/600 | Train Loss: 0.0158 | Val Loss: 0.0181\n",
      "Epoch 201/600 | Train Loss: 0.0165 | Val Loss: 0.0196\n",
      "Epoch 211/600 | Train Loss: 0.0146 | Val Loss: 0.0139\n",
      "Epoch 221/600 | Train Loss: 0.0172 | Val Loss: 0.0185\n",
      "Epoch 231/600 | Train Loss: 0.0145 | Val Loss: 0.0118\n",
      "Epoch 241/600 | Train Loss: 0.0161 | Val Loss: 0.0154\n",
      "Epoch 251/600 | Train Loss: 0.0148 | Val Loss: 0.0127\n",
      "Epoch 261/600 | Train Loss: 0.0171 | Val Loss: 0.0189\n",
      "Epoch 271/600 | Train Loss: 0.0151 | Val Loss: 0.0110\n",
      "Epoch 281/600 | Train Loss: 0.0156 | Val Loss: 0.0117\n",
      "Epoch 291/600 | Train Loss: 0.0145 | Val Loss: 0.0119\n",
      "Epoch 301/600 | Train Loss: 0.0153 | Val Loss: 0.0139\n",
      "Epoch 311/600 | Train Loss: 0.0127 | Val Loss: 0.0126\n",
      "Epoch 321/600 | Train Loss: 0.0140 | Val Loss: 0.0144\n",
      "Epoch 331/600 | Train Loss: 0.0131 | Val Loss: 0.0130\n",
      "Epoch 341/600 | Train Loss: 0.0121 | Val Loss: 0.0108\n",
      "Epoch 351/600 | Train Loss: 0.0121 | Val Loss: 0.0113\n",
      "Epoch 361/600 | Train Loss: 0.0120 | Val Loss: 0.0100\n",
      "Epoch 371/600 | Train Loss: 0.0131 | Val Loss: 0.0116\n",
      "Epoch 381/600 | Train Loss: 0.0123 | Val Loss: 0.0104\n",
      "Epoch 391/600 | Train Loss: 0.0106 | Val Loss: 0.0098\n",
      "Epoch 401/600 | Train Loss: 0.0106 | Val Loss: 0.0101\n",
      "Epoch 411/600 | Train Loss: 0.0120 | Val Loss: 0.0110\n",
      "Epoch 421/600 | Train Loss: 0.0132 | Val Loss: 0.0134\n",
      "Epoch 431/600 | Train Loss: 0.0109 | Val Loss: 0.0092\n",
      "Epoch 441/600 | Train Loss: 0.0099 | Val Loss: 0.0099\n",
      "Epoch 451/600 | Train Loss: 0.0109 | Val Loss: 0.0097\n",
      "Epoch 461/600 | Train Loss: 0.0102 | Val Loss: 0.0104\n",
      "Epoch 471/600 | Train Loss: 0.0103 | Val Loss: 0.0112\n",
      "Epoch 481/600 | Train Loss: 0.0149 | Val Loss: 0.0121\n",
      "Epoch 491/600 | Train Loss: 0.0099 | Val Loss: 0.0082\n",
      "Epoch 501/600 | Train Loss: 0.0110 | Val Loss: 0.0093\n",
      "Epoch 511/600 | Train Loss: 0.0099 | Val Loss: 0.0093\n",
      "Epoch 521/600 | Train Loss: 0.0093 | Val Loss: 0.0110\n",
      "Epoch 531/600 | Train Loss: 0.0091 | Val Loss: 0.0112\n",
      "Epoch 541/600 | Train Loss: 0.0086 | Val Loss: 0.0098\n",
      "Epoch 551/600 | Train Loss: 0.0094 | Val Loss: 0.0087\n",
      "Epoch 561/600 | Train Loss: 0.0095 | Val Loss: 0.0187\n",
      "Epoch 571/600 | Train Loss: 0.0091 | Val Loss: 0.0093\n",
      "Epoch 581/600 | Train Loss: 0.0102 | Val Loss: 0.0096\n",
      "Epoch 591/600 | Train Loss: 0.0104 | Val Loss: 0.0148\n",
      "Epoch 600/600 | Train Loss: 0.0090 | Val Loss: 0.0094\n",
      "Best validation loss with Xavier Initialization and Learning rate of 5e-4: 0.0070 at epoch 588\n"
     ]
    }
   ],
   "source": [
    "model_xavier_2 = LinearlyActuatedStrutsXavier()\n",
    "\n",
    "# Train and evaluate\n",
    "history_xavier_2 = train_model(\n",
    "    model=model_xavier_2,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    model_name=\"Xavier_Model_5e-4\",\n",
    "    num_epochs=600,\n",
    "    scale_factor=73.8,  # Your specific scaling factor\n",
    "    lr= 5e-4,  # Learning rate for the second model\n",
    ")\n",
    "\n",
    "# Access results\n",
    "print(f\"Best validation loss with Xavier Initialization and Learning rate of 5e-4: {history_xavier_2['best_val_loss']:.4f} at epoch {history_xavier_2['epoch_best']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Xavier_Model_5e-4 | MSE: 0.6662 | MAE: 0.4856\n"
     ]
    }
   ],
   "source": [
    "xavier_pred_2, xavier_mse_2, xavier_mae_2 = calculate_model_performance(\"Xavier_Model_5e-4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will try with He initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500 | Train Loss: 4.4049 | Val Loss: 1.9047\n",
      "Epoch 11/500 | Train Loss: 0.1512 | Val Loss: 0.1698\n",
      "Epoch 21/500 | Train Loss: 0.0678 | Val Loss: 0.0881\n",
      "Epoch 31/500 | Train Loss: 0.0424 | Val Loss: 0.0547\n",
      "Epoch 41/500 | Train Loss: 0.0354 | Val Loss: 0.0294\n",
      "Epoch 51/500 | Train Loss: 0.0330 | Val Loss: 0.0280\n",
      "Epoch 61/500 | Train Loss: 0.0466 | Val Loss: 0.0204\n",
      "Epoch 71/500 | Train Loss: 0.0207 | Val Loss: 0.0208\n",
      "Epoch 81/500 | Train Loss: 0.0191 | Val Loss: 0.0207\n",
      "Epoch 91/500 | Train Loss: 0.0209 | Val Loss: 0.0254\n",
      "Epoch 101/500 | Train Loss: 0.0148 | Val Loss: 0.0146\n",
      "Epoch 111/500 | Train Loss: 0.0177 | Val Loss: 0.0269\n",
      "Epoch 121/500 | Train Loss: 0.0168 | Val Loss: 0.0148\n",
      "Epoch 131/500 | Train Loss: 0.0147 | Val Loss: 0.0136\n",
      "Epoch 141/500 | Train Loss: 0.0127 | Val Loss: 0.0120\n",
      "Epoch 151/500 | Train Loss: 0.0122 | Val Loss: 0.0118\n",
      "Epoch 161/500 | Train Loss: 0.0107 | Val Loss: 0.0102\n",
      "Epoch 171/500 | Train Loss: 0.0186 | Val Loss: 0.0118\n",
      "Epoch 181/500 | Train Loss: 0.0138 | Val Loss: 0.0218\n",
      "Epoch 191/500 | Train Loss: 0.0109 | Val Loss: 0.0152\n",
      "Epoch 201/500 | Train Loss: 0.0152 | Val Loss: 0.0092\n",
      "Epoch 211/500 | Train Loss: 0.0097 | Val Loss: 0.0078\n",
      "Epoch 221/500 | Train Loss: 0.0099 | Val Loss: 0.0099\n",
      "Epoch 231/500 | Train Loss: 0.0092 | Val Loss: 0.0074\n",
      "Epoch 241/500 | Train Loss: 0.0097 | Val Loss: 0.0093\n",
      "Epoch 251/500 | Train Loss: 0.0106 | Val Loss: 0.0070\n",
      "Epoch 261/500 | Train Loss: 0.0080 | Val Loss: 0.0096\n",
      "Epoch 271/500 | Train Loss: 0.0096 | Val Loss: 0.0086\n",
      "Epoch 281/500 | Train Loss: 0.0080 | Val Loss: 0.0089\n",
      "Epoch 291/500 | Train Loss: 0.0101 | Val Loss: 0.0287\n",
      "Epoch 301/500 | Train Loss: 0.0113 | Val Loss: 0.0078\n",
      "Epoch 311/500 | Train Loss: 0.0077 | Val Loss: 0.0089\n",
      "Epoch 321/500 | Train Loss: 0.0089 | Val Loss: 0.0079\n",
      "Epoch 331/500 | Train Loss: 0.0073 | Val Loss: 0.0078\n",
      "Epoch 341/500 | Train Loss: 0.0089 | Val Loss: 0.0080\n",
      "Epoch 351/500 | Train Loss: 0.0083 | Val Loss: 0.0104\n",
      "Epoch 361/500 | Train Loss: 0.0078 | Val Loss: 0.0090\n",
      "Epoch 371/500 | Train Loss: 0.0078 | Val Loss: 0.0063\n",
      "Epoch 381/500 | Train Loss: 0.0073 | Val Loss: 0.0061\n",
      "Epoch 391/500 | Train Loss: 0.0081 | Val Loss: 0.0087\n",
      "Epoch 401/500 | Train Loss: 0.0065 | Val Loss: 0.0071\n",
      "Epoch 411/500 | Train Loss: 0.0086 | Val Loss: 0.0067\n",
      "Epoch 421/500 | Train Loss: 0.0071 | Val Loss: 0.0068\n",
      "Epoch 431/500 | Train Loss: 0.0063 | Val Loss: 0.0056\n",
      "Epoch 441/500 | Train Loss: 0.0065 | Val Loss: 0.0058\n",
      "Epoch 451/500 | Train Loss: 0.0064 | Val Loss: 0.0068\n",
      "Epoch 461/500 | Train Loss: 0.0072 | Val Loss: 0.0074\n",
      "Epoch 471/500 | Train Loss: 0.0091 | Val Loss: 0.0106\n",
      "Epoch 481/500 | Train Loss: 0.0138 | Val Loss: 0.0104\n",
      "Epoch 491/500 | Train Loss: 0.0075 | Val Loss: 0.0063\n",
      "Epoch 500/500 | Train Loss: 0.0067 | Val Loss: 0.0056\n",
      "Best validation loss with He Initialization: 0.0053 at epoch 489\n"
     ]
    }
   ],
   "source": [
    "model_he = LinearlyActuatedStrutsHe()\n",
    "# Train and evaluate\n",
    "history_he = train_model(\n",
    "    model=model_he,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    model_name=\"He_Model_1e-3\",\n",
    "    num_epochs=500,\n",
    "    scale_factor=73.8  # Your specific scaling factor\n",
    ")\n",
    "\n",
    "print(f\"Best validation loss with He Initialization: {history_he['best_val_loss']:.4f} at epoch {history_he['epoch_best']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: He_Model_1e-3 | MSE: 0.4668 | MAE: 0.4097\n",
      "tensor([[73.5000, 73.8000, 73.8000,  0.4000,  0.1000, 69.8000],\n",
      "        [ 0.0000,  0.1000, 58.5000,  0.0000, 48.9000,  0.0000],\n",
      "        [48.8000, 29.9000,  0.2000, 17.4000, 25.1000, 24.3000],\n",
      "        ...,\n",
      "        [73.8000, 73.6000, 73.6000, 53.7000,  0.1000,  0.1000],\n",
      "        [73.8000, 73.8000, 73.8000,  0.0000, 73.5000, 52.9000],\n",
      "        [73.7000, 69.7000,  0.1000, 44.0000, 66.6000, 55.7000]])\n"
     ]
    }
   ],
   "source": [
    "he_pred, he_mse, he_mae = calculate_model_performance(\"He_Model_1e-3\")\n",
    "\n",
    "he_pred = torch.round(he_pred, decimals=1)\n",
    "print(he_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/600 | Train Loss: 5.9647 | Val Loss: 3.1303\n",
      "Epoch 11/600 | Train Loss: 0.2495 | Val Loss: 0.2627\n",
      "Epoch 21/600 | Train Loss: 0.1110 | Val Loss: 0.1076\n",
      "Epoch 31/600 | Train Loss: 0.0603 | Val Loss: 0.0642\n",
      "Epoch 41/600 | Train Loss: 0.0463 | Val Loss: 0.0516\n",
      "Epoch 51/600 | Train Loss: 0.0348 | Val Loss: 0.0299\n",
      "Epoch 61/600 | Train Loss: 0.0361 | Val Loss: 0.0353\n",
      "Epoch 71/600 | Train Loss: 0.0253 | Val Loss: 0.0266\n",
      "Epoch 81/600 | Train Loss: 0.0260 | Val Loss: 0.0224\n",
      "Epoch 91/600 | Train Loss: 0.0240 | Val Loss: 0.0316\n",
      "Epoch 101/600 | Train Loss: 0.0191 | Val Loss: 0.0180\n",
      "Epoch 111/600 | Train Loss: 0.0191 | Val Loss: 0.0294\n",
      "Epoch 121/600 | Train Loss: 0.0211 | Val Loss: 0.0236\n",
      "Epoch 131/600 | Train Loss: 0.0157 | Val Loss: 0.0130\n",
      "Epoch 141/600 | Train Loss: 0.0150 | Val Loss: 0.0162\n",
      "Epoch 151/600 | Train Loss: 0.0156 | Val Loss: 0.0130\n",
      "Epoch 161/600 | Train Loss: 0.0144 | Val Loss: 0.0202\n",
      "Epoch 171/600 | Train Loss: 0.0164 | Val Loss: 0.0152\n",
      "Epoch 181/600 | Train Loss: 0.0155 | Val Loss: 0.0168\n",
      "Epoch 191/600 | Train Loss: 0.0136 | Val Loss: 0.0143\n",
      "Epoch 201/600 | Train Loss: 0.0143 | Val Loss: 0.0182\n",
      "Epoch 211/600 | Train Loss: 0.0124 | Val Loss: 0.0120\n",
      "Epoch 221/600 | Train Loss: 0.0132 | Val Loss: 0.0134\n",
      "Epoch 231/600 | Train Loss: 0.0112 | Val Loss: 0.0104\n",
      "Epoch 241/600 | Train Loss: 0.0124 | Val Loss: 0.0120\n",
      "Epoch 251/600 | Train Loss: 0.0137 | Val Loss: 0.0101\n",
      "Epoch 261/600 | Train Loss: 0.0136 | Val Loss: 0.0239\n",
      "Epoch 271/600 | Train Loss: 0.0123 | Val Loss: 0.0092\n",
      "Epoch 281/600 | Train Loss: 0.0110 | Val Loss: 0.0106\n",
      "Epoch 291/600 | Train Loss: 0.0104 | Val Loss: 0.0109\n",
      "Epoch 301/600 | Train Loss: 0.0112 | Val Loss: 0.0097\n",
      "Epoch 311/600 | Train Loss: 0.0102 | Val Loss: 0.0114\n",
      "Epoch 321/600 | Train Loss: 0.0108 | Val Loss: 0.0104\n",
      "Epoch 331/600 | Train Loss: 0.0094 | Val Loss: 0.0087\n",
      "Epoch 341/600 | Train Loss: 0.0100 | Val Loss: 0.0099\n",
      "Epoch 351/600 | Train Loss: 0.0093 | Val Loss: 0.0099\n",
      "Epoch 361/600 | Train Loss: 0.0091 | Val Loss: 0.0087\n",
      "Epoch 371/600 | Train Loss: 0.0096 | Val Loss: 0.0081\n",
      "Epoch 381/600 | Train Loss: 0.0083 | Val Loss: 0.0082\n",
      "Epoch 391/600 | Train Loss: 0.0086 | Val Loss: 0.0099\n",
      "Epoch 401/600 | Train Loss: 0.0086 | Val Loss: 0.0111\n",
      "Epoch 411/600 | Train Loss: 0.0095 | Val Loss: 0.0158\n",
      "Epoch 421/600 | Train Loss: 0.0086 | Val Loss: 0.0084\n",
      "Epoch 431/600 | Train Loss: 0.0083 | Val Loss: 0.0077\n",
      "Epoch 441/600 | Train Loss: 0.0077 | Val Loss: 0.0083\n",
      "Epoch 451/600 | Train Loss: 0.0078 | Val Loss: 0.0076\n",
      "Epoch 461/600 | Train Loss: 0.0076 | Val Loss: 0.0075\n",
      "Epoch 471/600 | Train Loss: 0.0082 | Val Loss: 0.0109\n",
      "Epoch 481/600 | Train Loss: 0.0074 | Val Loss: 0.0094\n",
      "Epoch 491/600 | Train Loss: 0.0082 | Val Loss: 0.0072\n",
      "Epoch 501/600 | Train Loss: 0.0080 | Val Loss: 0.0066\n",
      "Epoch 511/600 | Train Loss: 0.0078 | Val Loss: 0.0081\n",
      "Epoch 521/600 | Train Loss: 0.0072 | Val Loss: 0.0071\n",
      "Epoch 531/600 | Train Loss: 0.0073 | Val Loss: 0.0079\n",
      "Epoch 541/600 | Train Loss: 0.0072 | Val Loss: 0.0064\n",
      "Epoch 551/600 | Train Loss: 0.0074 | Val Loss: 0.0071\n",
      "Epoch 561/600 | Train Loss: 0.0077 | Val Loss: 0.0067\n",
      "Epoch 571/600 | Train Loss: 0.0068 | Val Loss: 0.0065\n",
      "Epoch 581/600 | Train Loss: 0.0068 | Val Loss: 0.0065\n",
      "Epoch 591/600 | Train Loss: 0.0063 | Val Loss: 0.0081\n",
      "Epoch 600/600 | Train Loss: 0.0069 | Val Loss: 0.0068\n",
      "Best validation loss with He Initialization and Learning rate of 5e-4: 0.0059 at epoch 582\n"
     ]
    }
   ],
   "source": [
    "he_model_2 = LinearlyActuatedStrutsHe()\n",
    "# Train and evaluate\n",
    "history_he_2 = train_model(\n",
    "    model=he_model_2,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    model_name=\"He_Model_5e-4\",\n",
    "    num_epochs=600,\n",
    "    scale_factor=73.8,  # Your specific scaling factor\n",
    "    lr= 5e-4,  # Learning rate for the second model\n",
    ")\n",
    "\n",
    "print(f\"Best validation loss with He Initialization and Learning rate of 5e-4: {history_he_2['best_val_loss']:.4f} at epoch {history_he_2['epoch_best']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: He_Model_5e-4 | MSE: 0.5009 | MAE: 0.4107\n",
      "tensor([[73.8000, 73.7000, 73.8000,  0.6000,  0.1000, 69.9000],\n",
      "        [ 0.0000,  0.0000, 59.2000,  0.0000, 50.3000,  0.0000],\n",
      "        [48.9000, 29.4000,  0.2000, 17.4000, 24.5000, 24.0000],\n",
      "        ...,\n",
      "        [73.6000, 73.6000, 73.6000, 54.4000,  0.1000,  0.0000],\n",
      "        [73.7000, 73.5000, 73.8000,  0.0000, 73.4000, 52.2000],\n",
      "        [73.7000, 70.2000,  0.1000, 45.2000, 67.5000, 57.1000]])\n"
     ]
    }
   ],
   "source": [
    "he_pred_2, he_mse_2, he_mae_2 = calculate_model_performance(\"He_Model_5e-4\")\n",
    "\n",
    "he_pred_2 = torch.round(he_pred_2, decimals=1)\n",
    "print(he_pred_2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
